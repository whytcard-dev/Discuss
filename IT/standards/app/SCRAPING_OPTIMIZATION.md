# Guida all'ottimizzazione dello scraping ## Introduzione Il web scraping è una componente fondamentale del progetto WhytCard, ma può richiedere molte risorse e presentare problemi di prestazioni. Questa guida illustra strategie e best practice per ottimizzare le operazioni di scraping, massimizzando l'efficienza e riducendo al minimo l'utilizzo delle risorse e l'impatto sui siti web di destinazione. ## Indice 1. [Principi fondamentali](#fundamental-principles) 2. [Architettura distribuita](#distributed-architecture) 3. [Ottimizzazione delle richieste HTTP](#http-request-optimization) 4. [Parallelizzazione e concorrenza](#parallelization-and-concurrency) 5. [Ottimizzazione dell'analisi HTML](#html-parsing-optimization) 6. [Gestione delle risorse](#resource-management) 7. [Monitoraggio e profilazione](#monitoring-and-profiling) 8. [Tecniche specializzate](#specialized-techniques) ## Principi fondamentali ### Efficienza vs. Cortesia L'ottimizzazione dello scraping deve bilanciare due obiettivi a volte contrastanti: 1. **Efficienza**: Massimizzazione della velocità di raccolta dati e dell'utilizzo delle risorse 2. **Cortesia**: Minimizzazione dell'impatto sui siti Web di destinazione e rispetto delle loro risorse Quando questi obiettivi sono in conflitto, dai sempre la priorità all'essere un buon cittadino del web piuttosto che alla pura prestazione. ### Metriche chiave Quando ottimizzi le operazioni di scraping, concentrati su queste metriche chiave: - **Pagine al minuto**: Velocità di raccolta delle pagine - **Utilizzo della CPU**: Sovraccarico di elaborazione - **Utilizzo della memoria**: Consumo di RAM - **Efficienza della rete**: Utilizzo della larghezza di banda - **Tasso di errore**: Percentuale di richieste non riuscite - **Impatto sul server di destinazione**: Carico sui siti sottoposti a scraping ## Architettura distribuita ### Distribuzione delle attività Per lo scraping su larga scala, distribuisci le attività tra più worker: ```python # Esempio di utilizzo di Celery per lo scraping distribuito from celery import Celery app = Celery('scraping_tasks', broker='redis://localhost:6379/0') @app.task def scrape_url(url): # Logica di scraping return result # Invio delle attività urls = ["https://example1.com", "https://example2.com", "https://example3.com"] results = [scrape_url.delay(url) per url in urls] ``` ### Bilanciamento del carico Implementa il bilanciamento del carico per distribuire le richieste su più indirizzi IP o istanze: ```python class LoadBalancer: def __init__(self, proxies): self.proxies = proxies self.current_index = 0 def get_next_proxy(self): proxy = self.proxies[self.current_index] self.current_index = (self.current_index + 1) % len(self.proxies) return proxy ``` ### Rotazione proxy Utilizza la rotazione proxy per evitare la limitazione della velocità basata su IP: ```python async def fetch_with_proxy_rotation(url, proxy_manager, session): max_retries = 3 retry_count = 0 while retry_count < max_retries: proxy = proxy_manager.get_next_proxy() prova: asincrono con session.get(url, proxy=proxy, timeout=30) come risposta: se response.status == 200: restituisci await response.text() elif response.status == 429: # Troppe richieste # Contrassegna questo proxy come con velocità limitata proxy_manager.mark_rate_limited(proxy) retry_count += 1 altrimenti: retry_count += 1 eccetto eccezione come e: # Contrassegna questo proxy come non riuscito proxy_manager.mark_failed(proxy) retry_count += 1 raise Exception(f"Impossibile recuperare {url} dopo {max_retries} tentativi") ``` ## Ottimizzazione delle richieste HTTP ### Pool di connessioni Riutilizza le connessioni HTTP per ridurre il sovraccarico: ```python async def scrape_with_connection_pooling(): # Crea una singola sessione per più richieste asincrone con aiohttp.ClientSession() come sessione: task = [] per url in urls: tasks.append(fetch(url, session)) return await asyncio.gather(*tasks) async def fetch(url, session): async con session.get(url) come risposta: return await response.text() ``` ### Supporto HTTP/2 Usa HTTP/2 quando disponibile per trarre vantaggio dal multiplexing: ```python import httpx async def fetch_with_http2(): async con httpx.AsyncClient(http2=True) come client: response = await client.get("https://example.com") return response.text ``` ### Compressione Richiedi risposte compresse per ridurre l'utilizzo della larghezza di banda: ```python headers = { 'Accept-Encoding': 'gzip, deflate, br', 'User-Agent': 'WhytCardBot/1.0' } async def fetch_with_compression(url, session): async con session.get(url, headers=headers) come risposta: return await response.text() ``` ### Ottimizzazione della richiesta Richiedi solo ciò di cui hai bisogno: ```python # Richiedi solo le intestazioni necessarie headers = { 'Accept': 'text/html', 'Accept-Language': 'en-US,en;q=0.5', 'Accept-Encoding': 'gzip, deflate, br', 'User-Agent': 'WhytCardBot/1.0' } # Usa le richieste HEAD per controllare le risorse prima di GET async def check_before_download(url, session): async con session.head(url) come risposta: if response.status == 200 and response.headers.get('Content-Type') == 'text/html': return await fetch_full_page(url, sessione) restituisci Nessuno ``` ## Parallelizzazione e concorrenza ### Scraping asincrono Usa la programmazione asincrona per gestire più richieste contemporaneamente: ```python import asyncio import aiohttp async def scrape_all(urls): async with aiohttp.ClientSession() as session: tasks = [scrape_one(url, session) for url in urls] return await asyncio.gather(*tasks, return_exceptions=True) async def scrape_one(url, session): try: async with session.get(url, timeout=30) as response: if response.status == 200: html = await response.text() return parse_html(html) else: restituisci Nessuno eccetto Exception as e: logger.error(f"Errore durante lo scraping di {url}: {e}") return Nessuno ``` ### Limite di concorrenza controllato concorrenza per evitare un impiego eccessivo di risorse: ```python async def scrape_with_semaphore(urls, max_concurrent=10): semaphore = asyncio.Semaphore(max_concurrent) async def _scrape_with_limit(url): async with semaphore: return await scrape_url(url) async with aiohttp.ClientSession() as session: tasks = [_scrape_with_limit(url) for url in urls] return await asyncio.gather(*tasks, return_exceptions=True) ``` ### Limitazione della velocità specifica per dominio Applica limiti di velocità diversi a domini diversi: ```python from urllib.parse import urlparse import time import asyncio class DomainRateLimiter: def __init__(self): # Dominio -> {last_request_time, requests_per_minute} self.domains = {} self.default_rpm = 30 # Predefinito: 30 richieste al minuto def set_domain_limit(self, domain, rpm): se il dominio non è in self.domains: self.domains[domain] = {"last_request_time": 0, "rpm": rpm} altrimenti: self.domains[domain]["rpm"] = rpm async def wait_if_needed(self, url): domain = urlparse(url).netloc se il dominio non è in self.domains: self.domains[domain] = {"last_request_time": 0, "rpm": self.default_rpm} domain_info = self.domains[domain] min_interval = 60.0 / domain_info["rpm"] current_time = time.time() elapsed = current_time - domain_info["last_request_time"] se trascorso < min_interval: wait_time = min_interval - elapsed await asyncio.sleep(wait_time) self.domains[domain]["last_request_time"] = time.time() ``` ## Ottimizzazione dell'analisi HTML ### Selezione del parser Scegli il parser più efficiente per le tue esigenze: ```python from bs4 import BeautifulSoup # lxml è molto più veloce di html.parser html = response.text soup = BeautifulSoup(html, 'lxml') ``` ### Estrazione mirata Utilizza selettori mirati invece di analizzare l'intero documento: ```python # Invece di analizzare tutto soup = BeautifulSoup(html, 'lxml') links = soup.find_all('a') # Utilizza selettori CSS per l'estrazione mirata links = soup.select('div.content a.external') # Oppure utilizza metodi di ricerca più specifici content_div = soup.find('div', class_='content') if content_div: links = content_div.find_all('a', class_='external') ``` ### Analisi in streaming Per documenti di grandi dimensioni, utilizzare gli analizzatori in streaming: ```python from lxml import etree def stream_parse_large_xml(file_path): """Analizza un file XML di grandi dimensioni senza caricarlo interamente in memoria.""" context = etree.iterparse(file_path, events=('end',), tag='item') for event, elem in context: # Elabora l'elemento process_element(elem) # Cancella l'elemento per liberare memoria elem.clear() # Elimina anche i riferimenti ora vuoti dal nodo radice a elem mentre elem.getprevious() non è None: del elem.getparent()[0] del context ``` ### Espressioni regolari per casi semplici Per estrazioni molto semplici, le espressioni regolari possono essere più veloci: ```python import re def extract_all_emails(text): """Estrai tutte le email dal testo utilizzando regex.""" email_pattern = r'[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\.[a-zA-Z]{2,}' return re.findall(email_pattern, text) ``` ## Gestione delle risorse ### Gestione della memoria Implementa strategie per ridurre al minimo l'utilizzo della memoria: ```python def process_large_dataset(file_path): """Elabora un set di dati di grandi dimensioni con un utilizzo minimo di memoria.""" # Utilizza generatori anziché elenchi with open(file_path, 'r') as f: for line in f: yield process_line(line) def scrape_with_memory_limit(urls, batch_size=100): """Esplora gli URL in batch per limitare l'utilizzo della memoria.""" for i in range(0, len(urls), batch_size): batch = urls[i:i+batch_size] results = scrape_batch(batch) process_and_save_results(results) # Libera memoria results = None ``` ### Caching su disco Memorizza le risposte nella cache sul disco per evitare richieste ridondanti: ```python import os import hashlib import pickle class DiskCache: def __init__(self, cache_dir='./cache'): self.cache_dir = cache_dir os.makedirs(cache_dir, exist_ok=True) def _get_cache_path(self, url): """Genera un percorso file per memorizzare nella cache il contenuto dell'URL.""" url_hash = hashlib.md5(url.encode()).hexdigest() return os.path.join(self.cache_dir, url_hash) def get(self, url): """Recupera il contenuto dalla cache se esiste.""" cache_path = self._get_cache_path(url) if os.path.exists(cache_path): with open(cache_path, 'rb') as f: return pickle.load(f) return None def set(self, url, content): """Memorizza il contenuto nella cache.""" cache_path = self._get_cache_path(url) with open(cache_path, 'wb') as f: pickle.dump(content, f) ``` ### Elaborazione incrementale Elabora i dati in modo incrementale per evitare picchi di memoria: ```python def incremental_scrape_and_process(urls): """Esplora ed elabora gli URL in modo incrementale.""" for url in urls: html = scrape_url(url) if html: data = extract_data(html) process_data(data) save_data(data) # Libera memoria html = None data = None ``` ## Monitoraggio e profilazione ### Misure di prestazione Tieni traccia delle metriche chiave di prestazione: ```python import time from dataclasses import dataclass from typing import Dict, List @dataclass class ScrapingMetrics: start_time: float = 0 end_time: float = 0 urls_processed: int = 0 successful_requests: int = 0 failed_requests: int = 0 bytes_downloaded: int = 0 domain_stats: Dict[str, Dict] = None def __post_init__(self): se self.domain_stats è None: self.domain_stats = {} def start(self): self.start_time = time.time() def stop(self): self.end_time = time.time() def add_request(self, url, success, size=0): from urllib.parse import urlparse domain = urlparse(url).netloc se il dominio non è in self.domain_stats: self.domain_stats[domain] = { 'richieste': 0, 'riuscito': 0, 'fallito': 0, 'byte': 0 } self.urls_processed += 1 self.domain_stats[domain]['richieste'] += 1 se ha successo: self.successful_requests += 1 self.domain_stats[domain]['riuscito'] += 1 self.bytes_downloaded += dimensione self.domain_stats[domain]['byte'] += dimensione altrimenti: self.failed_requests += 1 self.domain_stats[domain]['fallito'] += 1 def get_summary(self): duration = self.end_time - self.start_time se self.end_time > 0 altrimenti time.time() - self.start_time return { 'duration_seconds': durata, ``` ### Profilazione Profila il tuo codice per identificare i colli di bottiglia: ```python import cProfile import pstats import io def profile_function(func, *args, **kwargs): """Profila una funzione e stampa statistiche.""" pr = cProfile.Profile() pr.enable() risultato = func(*args, **kwargs) pr.disable() s = io.StringIO() ps = pstats.Stats(pr, stream=s).sort_stats('cumulative') ps.print_stats(20) # Stampa le prime 20 funzioni in base al tempo cumulativo print(s.getvalue()) return result # Utilizzo profile_function(scrape_batch, urls) ``` ### Registrazione Implementa la registrazione dettagliata per l'analisi: ```python import logging import time # Configura la registrazione logging.basicConfig( level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s', filename='scraping.log' ) logger = logging.getLogger('scraper') async def scrape_with_logging(url, session): start_time = time.time() logger.info(f"Richiesta iniziale su {url}") try: async con session.get(url) come risposta: duration = time.time() - start_time size = len(await response.read()) logger.info( f"Completato {url} - Stato: {response.status}, " f"Dimensione: {size} byte, Tempo: {duration:.2f}s" ) return await response.text() eccetto Eccezione come e: duration = time.time() - start_time logger.error(f"Non riuscito {url} - Errore: {str(e)}, Tempo: {duration:.2f}s") raise ``` ## Tecniche specializzate ### Browser headless per siti con JavaScript pesante Utilizza browser headless per siti che richiedono JavaScript: ```python from playwright.async_api import async_playwright async def scrape_js_site(url): async con async_playwright() come p: browser = await p.chromium.launch(headless=True) page = await browser.new_page() # Imposta il timeout page.set_default_timeout(30000) # Naviga e attendi l'inattività della rete await page.goto(url, wait_until='networkidle') # Estrai il contenuto content = await page.content() # Chiudi il browser await browser.close() restituisci content ``` ### Scansione intelligente Implementa strategie di scansione intelligenti: ```python class PriorityCrawler: def __init__(self): self.visited = set() self.queue = [] # (priority, url) tuple def add_url(self, url, priority=0): se l'URL non è in self.visited: import heapq heapq.heappush(self.queue, (-priority, url)) # Negativo per max-heap async def crawl(self, session, max_urls=100): results = {} count = 0 import heapq while self.queue and count < max_urls: _, url = heapq.heappop(self.queue) if url in self.visited: continue self.visited.add(url) count += 1 try: html = await self.fetch_url(url, session) results[url] = html # Estrai e assegna priorità ai nuovi link new_urls = self.extract_links(html, url) for new_url, priority in new_urls: self.add_url(new_url, priority) except Exception as e: logger.error(f"Errore durante la scansione di {url}: {e}") return results def extract_links(self, html, base_url): # Estrai i link e assegna priorità in base alla pertinenza # Restituisci elenco di tuple (url, priorità) passa ``` ### Limitazione basata sul contenuto Regola la velocità di scraping in base al tipo di contenuto: ```python async def adaptive_scrape(url, session): """Adatta il comportamento di scraping in base al tipo di contenuto.""" # Per prima cosa effettua una richiesta HEAD per controllare il tipo di contenuto asincrono con session.head(url) come head_response: content_type = head_response.headers.get('Content-Type', '') content_length = int(head_response.headers.get('Content-Length', 0)) # Regola il comportamento in base al contenuto if 'text/html' in content_type: # Pagina HTML standard await asyncio.sleep(1) # Ritardo standard elif 'application/json' in content_type: # Endpoint API - può essere più veloce await asyncio.sleep(0.5) elif content_length > 1000000: # Grande file - sii più cauto await asyncio.sleep(5) else: # Comportamento predefinito await asyncio.sleep(2) # Ora rendi la richiesta effettiva asincrona con session.get(url) come risposta: return await response.text() ``` ## Conclusione L'ottimizzazione delle operazioni di scraping è un equilibrio tra prestazioni, utilizzo delle risorse e considerazioni etiche. Implementando le tecniche descritte in questa guida, puoi creare sistemi di scraping efficienti che raccolgono dati in modo efficace riducendo al minimo l'impatto sui siti web di destinazione e sulle tue risorse. Ricorda che lo scraper più efficiente è quello che: 1. Raccoglie solo ciò di cui ha bisogno 2. Rispetta le risorse del sito web di destinazione 3. Utilizza le risorse di calcolo in modo efficiente 4. Gestisce gli errori in modo elegante 5. Si adatta alle condizioni mutevoli Monitora sempre le tue operazioni di scraping e sii pronto ad adattare il tuo approccio in base alle metriche delle prestazioni e al feedback dei siti web di destinazione. ---Ultimo aggiornamento: 15/01/2025