# Посібник з етичного парсингу для WhytCard

## Вступ

Веб-парсинг є основою проекту WhytCard, але він має проводитися етично, відповідально та законно. Цей посібник визначає принципи та практики, яких слід дотримуватися, щоб забезпечити дотримання прав власників веб-сайтів, чинного законодавства та етичних стандартів у всіх видах парсингу.

## Зміст

1. [Основні принципи](#fundamental-principles)
2. [Юридичні аспекти](#legal-aspects)
3. [Найкращі технічні практики](#technical-best-practices)
4. [Повага до ресурсів](#resource-respect)
5. [Захист персональних даних](#personal-data-protection)
6. [Документація та прозорість](#documentation-and-transparency)
7. [Особливі випадки](#special-cases)
8. [Контрольний список етичного скрейпінгу](#ethical-scraping-checklist)

## Основні принципи

### Філософія етичного скрейпінгу

Етичний скрейпінг базується на трьох фундаментальних принципах:

1. **Повага**: Поважайте власників веб-сайтів, їхні умови використання та їхні ресурси
2. **Пропорційність**: Витягуйте лише необхідні дані з мінімальним впливом
3. **Прозорість**: Будьте прозорими щодо особи бота та намірів парсингу

### Цінності WhytCard щодо парсингу

Як проект WhytCard, ми зобов'язуємося:

- Ніколи не завдавати шкоди веб-сайтам, які ми парсимо
- Суворо дотримуватися явних та неявних правил веб-сайтів
- Бути прозорими щодо нашої ідентичності та цілей
- Використовувати дані відповідально та відповідно до нашої місії
- Надавати пріоритет офіційним API, коли вони доступні

## Правові аспекти

### Загальна правова база

Веб-парсинг регулюється кількома правовими базами, які відрізняються залежно від країни:

- **Авторське право**: Контент веб-сайту загалом захищений авторським правом
- **Умови використання**: Умови використання веб-сайту можуть прямо забороняти парсинг
- **Захист даних**: Закони, такі як GDPR у Європі, захищають персональні дані
- **Несанкціонований доступ**: Деякі юрисдикції криміналізують несанкціонований доступ до комп'ютерних систем

### Визначна судова практика

Деякі важливі судові рішення щодо парсингу:

- **hiQ Labs проти LinkedIn** (США): Встановлено, що Збір публічних даних не обов'язково є незаконним
- **Ryanair проти PR Aviation** (ЄС): Підтверджено, що умови використання можуть контрактно обмежувати збір даних
- **QVC проти Resultly** (США): Наголошено на важливості не перевантажувати сервери

### Дотримання правових норм для WhytCard

Щоб залишатися в рамках закону:

1. **Завжди перевіряйте Умови використання** перед збіром даних з сайту

2. **Поважайте теги "noindex" та "nofollow"** у метатегах

3. **Ніколи не обходьте заходи технічного захисту** (CAPTCHA, обмеження доступу)

4. **Документуйте свої дії**, щоб продемонструвати добросовісність

5. **Зверніться до юриста**, якщо сумніваєтеся щодо законності операції збіру даних

## Найкращі технічні практики

### Дотримання robots.txt

Файл robots.txt визначає правила доступу для роботів:

```python
from urllib.robotparser import RobotFileParser
from urllib.parse import urlparse 

def is_allowed(url, user_agent="WhytCardBot/1.0"): 
"""Перевіряє, чи можна зібрати URL-адресу згідно з robots.txt.""" 
rp = RobotFileParser() 
rp.set_url(f"{urlparse(url).scheme}://{urlparse(url).netloc}/robots.txt") 
rp.read() 
return rp.can_fetch(user_agent, url) 
``` 

### Правильна ідентифікація 

Завжди використовуйте User-Agent, який чітко ідентифікує вашого бота: 

```python 
headers = { 
'User-Agent': 'WhytCardBot/1.0 (+https://whytcard.com/bot; bot@whytcard.com)', 
# Інші заголовки... 
} 
``` 

### Затримки запитів 

Реалізуйте розумні затримки між requests: 

```python 
import time 
import random 

def polite_request(url, session, min_delay=1, max_delay=3): 
"""Здійснює запит із ввічливою затримкою між запитами.""" 
# Очікування випадкової затримки 
delay = random.uniform(min_delay, max_delay) 
time.sleep(delay) 

# Здійснення запиту 
response = session.get(url, headers=headers) 
return response 
``` 

### Обробка помилок 

Поважайте коди помилок HTTP та відповідно адаптуйте свою поведінку: 

```python 
async def respectful_fetch(url, session): 
"""Отримує URL-адресу в шанобливій манері.""" 
try: 
async with session.get(url, headers=headers) as response: 
if response.status == 200: 
return await response.text() 
elif response.status == 429: # Забагато запитів # Зачекайте довше перед повторною спробою wait_time = int(response.headers.get('Retry-After', 60)) logger.info(f"Частота обмежена, очікування {wait_time} секунд") await asyncio.sleep(wait_time) return await respectful_fetch(url, session) elif response.status in (403, 404): # Не повторювати помилки 403/404 logger.warning(f"Доступ заборонено або не знайдено: {url}") return None else: # Зачекайте та повторіть спробу для інших помилок logger.warning(f"Помилка {response.status} для {url}, повторна спроба через 5 секунд") await asyncio.sleep(5) return await respectful_fetch(url, session) except Exception as e: logger.error(f"Виняток під час отримання {url}: {str(e)}") 
повернути Немає 
``` 

## Повага до ресурсів 

### Обмеження швидкості 

Адаптуйте швидкість запитів до розміру та ресурсів цільового сайту: 

- **Великі комерційні сайти**: 1 запит кожні 1-3 секунди 
- **Середні сайти**: 1 запит кожні 3-10 секунд 
- **Малі сайти**: 1 запит кожні 10-60 секунд або більше 

### Періоди парсингу 

Надавайте перевагу періодам з низьким трафіком для інтенсивних операцій: 

- **Позапікові години**: Надавайте перевагу ночам або вихідним 
- **Уникайте піків**: Не парьте під час відомих пікових періодів 
- **Будьте адаптивними**: Зменшуйте швидкість, якщо ви виявите уповільнення 

### Мінімізація впливу 

Методи зменшення впливу на цільові сервери: 

1. **Розумне кешування**: Не завантажуйте одну й ту саму сторінку кілька разів 
2. **Вибірковість**: Отримуйте лише ті сторінки, які вам дійсно потрібні
3. **Стиснення**: Запитуйте стиснуті відповіді для зменшення пропускної здатності
4. **Ефективна пагінація**: Дотримуйтесь структури пагінації сайту

## Захист персональних даних

### Виявлення персональних даних

Будьте пильними щодо типів даних, які ви збираєте:

- **Дані прямої ідентифікації**: Імена, електронні адреси, телефони, адреси
- **Дані непрямої ідентифікації**: Ідентифікатори користувачів, псевдоніми
- **Конфіденційні дані**: Політичні погляди, стан здоров'я, сексуальна орієнтація

### Принципи GDPR, яких слід дотримуватися

Якщо ви працюєте в Європі або збираєте дані від європейців:

1. **Мінімізація**: Збирайте лише суворо необхідні дані
2. **Мета**: Використовуйте дані лише за призначенням
3. **Обмежене зберігання**: Видаляйте дані, коли вони більше не потрібні
4. **Безпека**: Захищайте зібрані дані від несанкціонованого доступу

### Анонімізація даних

Методи анонімізації персональних даних data: 

```python 
import hashlib 
import re 

def anonymize_email(email): 
"""Анонімізує адресу електронної пошти.""" 
if not email: 
return None 

# Хешує адресу електронної пошти 
hashed = hashlib.sha256(email.encode()).hexdigest()[:10] 
domain = email.split('@')[-1] 

return f"anon_{hashed}@{domain}" 

def anonymize_phone(phone): 
"""Анонімізує номер телефону.""" 
if not phone: 
return None 

# Зберігає лише цифри 
digits = re.sub(r'\D', '', phone) 

# Маскує всі цифри, крім останніх 2 
if len(digits) > 2: 
return "X" * (len(digits) - 2) + цифри[-2:] 
повернути "X" * len(цифри) 
``` 

## Документація та прозорість 

### Документування діяльності зі збору даних 

Завжди документуйте свою діяльність зі збору даних: 

- **Мета**: Чому збираються ці дані? 
- **Метод**: Як вони збираються? 
- **Зберігання**: Де і як вони зберігаються? 
- **Використання**: Як вони будуть використані? 
- **Видалення**: Коли вони будуть видалені? 

### Контакти та відмова від розсилки

Завжди надавайте спосіб зв'язку з вами:

1. **Інформаційна сторінка**: Створіть окрему сторінку з поясненням вашого бота (наприклад, whytcard.com/bot)
2. **Контактна електронна адреса**: Вкажіть адресу електронної пошти у вашому User-Agent
3. **Механізм відмови від розсилки**: Дозвольте сайтам запитувати виключення

### Реєстрація активності

Ведіть детальні журнали вашої діяльності зі скрейпінгу:

```python 
import logging 
from datetime import datetime

# Конфігурація логера 
logging.basicConfig( 
filename=f"scraping_log_{datetime.now().strftime('%Y%m%d')}.log", 
level=logging.INFO, 
format='%(asctime)s - %(levelname)s - %(message)s' 
) 

def log_scraping_activity(url, success, data_points=0): 
"""Реєструє активність парсингу.""" 
logging.info(f"URL: {url}, Success: {success}, Data points: {data_points}") 
``` 

## Особливі випадки 

### API проти парсингу 

Порядок пріоритетів для збору даних: 

1. **Офіційні API**: Завжди надавайте пріоритет офіційним API, коли вони існують 
2. **Загальнодоступні канали даних**: Використовуйте канали RSS, XML або JSON, якщо вони доступні 
3. **Парсинг**: Використовуйте парсинг лише як крайній засіб 

### Сайти з автентифікацією 

Для сайтів, що потребують автентифікації: 

- **Явна авторизація**: Отримайте письмову авторизацію від сайту 
- **Дотримання Умов обслуговування**: Переконайтеся, що Умови обслуговування дозволяють автоматизоване використання 
- **Обмеження**: Суворо дотримуйтесь обмежень використання 

### Динамічний контент (JavaScript) 

Для сайтів, що використовують багато JavaScript: 

```python 
from drammar.async_api import async_playwright 

async def scrape_dynamic_content(url): 
"""Видобути контент, згенерований JavaScript.""" 
async with async_playwright() as p: 
browser = await p.chromium.launch(headless=True) 
page = await browser.new_page() 

# Налаштувати User-Agent 
await page.set_extra_http_headers({ 
'User-Agent': 'WhytCardBot/1.0 (+https://whytcard.com/bot)' 
}) 

# Завантажити сторінку та зачекати, поки мережа буде неактивна 
await page.goto(url) 
await page.wait_for_load_state('networkidle') 

# Видобути контент 
content = await page.content() 

await browser.close() 
return content 
``` 

## Контрольний список етичного скрапінгу 

Перед кожним проектом скрапінгу перевірте наступні пункти:

### Підготовка
- [ ] Перевірка Умов обслуговування цільового сайту
- [ ] Перевірка файлу robots.txt
- [ ] Пошук API або альтернатив парсингу
- [ ] Чітке визначення необхідних даних
- [ ] Документація мети парсингу

### Технічна конфігурація
- [ ] Ідентифікований та прозорий User-Agent
- [ ] Механізм обмеження швидкості
- [ ] Система кешування для уникнення надлишкових запитів
- [ ] Належна обробка помилок та HTTP-кодів
- [ ] Журнал активності

### Виконання
- [ ] Моніторинг продуктивності цільового сайту
- [ ] Динамічне коригування швидкості за потреби
- [ ] Дотримання вказівок сервера (429, Retry-After)
- [ ] Негайне зупинення у разі виявлення проблеми

### Післяобробка
- [ ] Анонімізація персональних даних
- [ ] Безпечне зберігання даних
- [ ] Обмежене за часом зберігання
- [ ] Документація зібраних даних дані 

## Висновок 

Етичний парсинг – це баланс між доступом до даних та повагою до прав і ресурсів власників веб-сайтів. Дотримуючись цих принципів і практик, проект WhytCard може збирати необхідні дані, зберігаючи при цьому відповідальний та шанобливий підхід.

Пам’ятайте, що етика парсингу – це не лише питання дотримання законодавства, а й відповідальність перед веб-екосистемою в цілому. Шанобливий парсинг сприяє більш відкритому та сталому вебу для всіх.

--- 

Останнє оновлення: 2025-01-15