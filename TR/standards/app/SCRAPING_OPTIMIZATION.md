# Scraping Optimizasyon Kılavuzu ## Giriş Web scraping, WhytCard projesinin temel bir bileşenidir, ancak kaynak yoğun olabilir ve performans zorlukları sunabilir. Bu kılavuz, kaynak kullanımını ve hedef web sitelerindeki etkiyi en aza indirirken verimliliği en üst düzeye çıkarmak için scraping işlemlerini optimize etmeye yönelik stratejileri ve en iyi uygulamaları ana hatlarıyla açıklamaktadır. ## İçindekiler 1. [Temel İlkeler](#fundamental-principles) 2. [Dağıtılmış Mimari](#distributed-architecture) 3. [HTTP İstek Optimizasyonu](#http-istek-optimizasyonu) 4. [Paralelleştirme ve Eşzamanlılık](#paralelleştirme-ve-eşzamanlılık) 5. [HTML Ayrıştırma Optimizasyonu](#html-parsing-optimization) 6. [Kaynak Yönetimi](#kaynak-yönetimi) 7. [İzleme ve Profilleme](#monitoring-and-profiling) 8. [Özel Teknikler](#specialized-techniques) ## Temel İlkeler ### Verimlilik ve Nezaket Kazıma optimizasyonu bazen çelişen iki hedefi dengelemelidir: 1. **Verimlilik**: Veri toplama hızını ve kaynak kullanımını en üst düzeye çıkarma 2. **Nezaket**: Hedef web siteleri üzerindeki etkiyi en aza indirin ve kaynaklarına saygı gösterin. Bu hedefler çeliştiğinde, salt performanstan ziyade her zaman iyi bir web vatandaşı olmaya öncelik verin. ### Temel Ölçütler Kazıma işlemlerini optimize ederken, şu temel ölçütlere odaklanın: - **Dakika başına sayfa**: Sayfa toplama oranı - **CPU kullanımı**: İşleme yükü - **Bellek kullanımı**: RAM tüketimi - **Ağ verimliliği**: Bant genişliği kullanımı - **Hata oranı**: Başarısız istek yüzdesi - **Hedef sunucu etkisi**: Kazınan sitelere yüklenen yük ## Dağıtılmış Mimari ### Görev Dağıtımı Büyük ölçekli kazıma için görevleri birden fazla çalışana dağıtın: ```python # Dağıtılmış kazıma için Celery kullanma örneği from celery import Celery app = Celery('scraping_tasks', broker='redis://localhost:6379/0') @app.task def scrape_url(url): # Kazıma mantığı return result # Görevleri gönder urls = ["https://example1.com", "https://example2.com", "https://example3.com"] results = [scrape_url.delay(url) for url in urls] ``` ### Yük Dengeleme İstekleri birden fazla IP adresine veya örneğe dağıtmak için yük dengelemeyi uygulayın: ```python class LoadBalancer: def __init__(self, proxies): self.proxies = proxies self.current_index = 0 def get_next_proxy(self): proxy = self.proxies[self.current_index] self.current_index = (self.current_index + 1) % len(self.proxies) return proxy ``` ### Proxy Rotasyonu IP tabanlı hız sınırlamasını önlemek için proxy rotasyonunu kullanın: ```python async def fetch_with_proxy_rotation(url, proxy_manager, session): max_retries = 3 retry_count = 0 while retry_count < max_retries: proxy = proxy_manager.get_next_proxy() try: async with session.get(url, proxy=proxy, timeout=30) as response: if response.status == 200: return await response.text() elif response.status == 429: # Çok Fazla İstek # Bu proxy'yi hız sınırlı olarak işaretle proxy_manager.mark_rate_limited(proxy) retry_count += 1 else: retry_count += 1 except Exception as e: # Bu proxy'yi başarısız olarak işaretle proxy_manager.mark_failed(proxy) retry_count += 1 raise Exception(f"{max_retries} denemeden sonra {url} getirilemedi") ``` ## HTTP İstek Optimizasyonu ### Bağlantı Havuzu Yükü azaltmak için HTTP bağlantılarını yeniden kullan: ```python async def scrape_with_connection_pooling(): # Oluştur birden fazla istek için tek bir oturum async aiohttp.ClientSession() as session: tasks = [] for url in urls: tasks.append(fetch(url, session)) return await asyncio.gather(*tasks) async def fetch(url, session): async with session.get(url) as response: return await response.text() ``` ### HTTP/2 Desteği Çoklamadan faydalanmak için HTTP/2'yi kullanılabilir olduğunda kullanın: ```python import httpx async def fetch_with_http2(): async with httpx.AsyncClient(http2=True) as client: response = await client.get("https://example.com") return response.text ``` ### Sıkıştırma Bant genişliği kullanımını azaltmak için sıkıştırılmış yanıtlar isteyin: ```python headers = { 'Accept-Encoding': 'gzip, deflate, br', 'Kullanıcı Aracısı': 'WhytCardBot/1.0' } async def fetch_with_compression(url, session): async with session.get(url, headers=headers) as response: return await response.text() ``` ### İstek Optimizasyonu Yalnızca ihtiyacınız olanı isteyin: ```python # Yalnızca gerekli başlıkları isteyin headers = { 'Accept': 'text/html', 'Accept-Language': 'en-US,en;q=0.5', 'Accept-Encoding': 'gzip, deflate, br', 'Kullanıcı Aracısı': 'WhytCardBot/1.0' } # GET'ten önce kaynakları kontrol etmek için HEAD isteklerini kullanın async def check_before_download(url, session): async with session.head(url) as response: if response.status == 200 ve response.headers.get('Content-Type') == 'text/html': return await fetch_full_page(url, session) return None ``` ## Paralelleştirme ve Eşzamanlılık ### Eşzamansız Kazıma Birden fazla isteği eş zamanlı olarak işlemek için eşzamansız programlamayı kullanın: ```python import asyncio import aiohttp async def scrape_all(urls): async with aiohttp.ClientSession() as session: tasks = [scrape_one(url, session) for url in urls] return await asyncio.gather(*tasks, return_exceptions=True) async def scrape_one(url, session): try: async with session.get(url, timeout=30) as response: if response.status == 200: html = await response.text() return parse_html(html) else: return None except Exception as e: logger.error(f"{url}: {e} kazıma hatası") return None ``` ### Kontrollü Eşzamanlılık Kaynakların aşırı yüklenmesini önlemek için eşzamanlılığı sınırlayın: ```python async def scrape_with_semaphore(urls, max_concurrent=10): semaphore = asyncio.Semaphore(max_concurrent) async def _scrape_with_limit(url): async with semaphore: return await scrape_url(url) async with aiohttp.ClientSession() as session: tasks = [_scrape_with_limit(url) for url in urls] return await asyncio.gather(*tasks, return_exceptions=True) ``` ### Alana Özgü Oran Sınırlaması Farklı alanlara farklı oran sınırları uygulayın: ```python from urllib.parse import urlparse import time asyncio sınıfı DomainRateLimiter'ı içe aktar: def __init__(self): # Alan Adı -> {son_istek_zamanı, dakika_başına_istek} self.domains = {} self.default_rpm = 30 # Varsayılan: Dakikada 30 istek def set_domain_limit(self, domain, rpm): alan adı self.domains'de değilse: self.domains[domain] = {"son_istek_zamanı": 0, "rpm": rpm} değilse: self.domains[domain]["rpm"] = rpm async def wait_if_needed(self, url): alan adı = urlparse(url).netloc alan adı self.domains'de değilse: self.domains[domain] = {"son_istek_zamanı": 0, "rpm": self.default_rpm} alan_adı_bilgisi = self.domains[domain] min_interval = 60.0 / domain_info["rpm"] current_time = time.time() elapsed = current_time - domain_info["last_request_time"] if elapsed < min_interval: wait_time = min_interval - elapsed await asyncio.sleep(wait_time) self.domains[domain]["last_request_time"] = time.time() ``` ## HTML Ayrıştırma Optimizasyonu ### Ayrıştırıcı Seçimi İhtiyaçlarınız için en verimli ayrıştırıcıyı seçin: ```python from bs4 import BeautifulSoup # lxml, html.parser'dan çok daha hızlıdır html = response.text soup = BeautifulSoup(html, 'lxml') ``` ### Hedeflenen Çıkarım Tüm belgeyi ayrıştırmak yerine hedeflenen seçicileri kullanın: ```python # Her şeyi ayrıştırmak yerine soup = BeautifulSoup(html, 'lxml') links = soup.find_all('a') # Hedeflenen çıkarma işlemleri için CSS seçicileri kullanın bağlantılar = soup.select('div.content a.external') # Veya daha spesifik bulma yöntemleri kullanın content_div = soup.find('div', class_='content') if content_div: links = content_div.find_all('a', class_='external') ``` ### Akışlı Ayrıştırma Büyük belgeler için akışlı ayrıştırıcıları kullanın: ```python from lxml import etree def stream_parse_large_xml(file_path): """Büyük bir XML dosyasını tamamen belleğe yüklemeden ayrıştırın.""" context = etree.iterparse(file_path, events=('end',), tag='item') for event, elem in context: # Öğeyi işleyin process_element(elem) # Belleği boşaltmak için öğeyi temizleyin elem.clear() # Ayrıca, elem.getprevious() None olmadığında kök düğümden elem'e olan artık boş referansları ortadan kaldırın: del elem.getparent()[0] del context ``` ### Basit Durumlar İçin Düzenli İfadeler Çok basit çıkarmalar için, regex daha hızlı olabilir: ```python import re def extract_all_emails(text): """Regex kullanarak metinden tüm e-postaları ayıkla.""" email_pattern = r'[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\.[a-zA-Z]{2,}' return re.findall(email_pattern, text) ``` ## Kaynak Yönetimi ### Bellek Yönetimi Bellek kullanımını en aza indirmek için stratejiler uygulayın: ```python def process_large_dataset(file_path): """Minimum bellek kullanımıyla büyük bir veri kümesini işleyin.""" # Listeler yerine üreteçleri kullanın open(file_path, 'r') as f: for line in f: yield process_line(line) def scrape_with_memory_limit(urls, batch_size=100): """Bellek kullanımını sınırlamak için URL'leri gruplar halinde kazı.""" for i in range(0, len(urls), batch_size): batch = urls[i:i+batch_size] results = scrape_batch(batch) process_and_save_results(results) # Belleği boşalt results = None ``` ### Disk Önbelleğe Alma Gereksiz isteklerden kaçınmak için yanıtları diske önbelleğe al: ```python import os import hashlib import pickle class DiskCache: def __init__(self, cache_dir='./cache'): self.cache_dir = cache_dir os.makedirs(cache_dir, exist_ok=True) def _get_cache_path(self, url): """URL içeriğini önbelleğe almak için bir dosya yolu oluştur.""" url_hash = hashlib.md5(url.encode()).hexdigest() return os.path.join(self.cache_dir, url_hash) def get(self, url): """İçeriği varsa önbellekten al.""" cache_path = self._get_cache_path(url) if os.path.exists(cache_path): with open(cache_path, 'rb') as f: return pickle.load(f) return None def set(self, url, content): """İçeriği önbellekte sakla.""" cache_path = self._get_cache_path(url) with open(cache_path, 'wb') as f: pickle.dump(content, f) ``` ### Artımlı İşleme Bellek artışlarını önlemek için verileri artımlı olarak işleyin: ```python def incremental_scrape_and_process(urls): """Kazı ve URL'leri artımlı olarak işleyin.""" for url in urls: html = scrape_url(url) if html: data = extract_data(html) process_data(data) save_data(data) # Belleği boşalt html = None data = None ``` ## İzleme ve Profilleme ### Performans Ölçümleri Önemli performans ölçümlerini izleyin: ```python import time from dataclasses import dataclass from typing import Dict, List @dataclass class ScrapingMetrics: başlangıç_zamanı: float = 0 bitiş_zamanı: float = 0 urls_processed: int = 0 successful_requests: int = 0 failed_requests: int = 0 bytes_downloaded: int = 0 domain_stats: Dict[str, Dict] = None def __post_init__(self): if self.domain_stats is None: self.domain_stats = {} def start(self): self.start_time = time.time() def stop(self): self.end_time = time.time() def add_request(self, url, successful, size=0): from urllib.parse import urlparse domain = urlparse(url).netloc if domain not in self.domain_stats: self.domain_stats[domain] = { 'requests': 0, 'successful': 0, 'failed': 0, 'bytes': 0 } self.urls_processed += 1 self.domain_stats[domain]['requests'] += 1 if successful: self.successful_requests += 1 self.domain_stats[domain]['successful'] += 1 self.bytes_downloaded += size self.domain_stats[domain]['bytes'] += size else: self.failed_requests += 1 self.domain_stats[domain]['başarısız'] += 1 def get_summary(self): süre = self.bitiş_zamanı - self.başlangıç_zamanı eğer self.bitiş_zamanı > 0 değilse time.time() - self.başlangıç_zamanı return { 'süre_saniye': süre, 'işlenen_url'ler': self.url'ler_işlendi, 'başarılı_istekler': self.başarılı_istekler, 'başarısız_istekler': self.başarısız_istekler, 'başarı_oranı': self.başarılı_istekler / maks(1, self.url'ler_işlendi), 'saniye_başına_istekler': self.url'ler_işlendi / maks(1, süre), 'indirilen_bayt': self.indirilen_bayt, 'indirme_oranı_kbps': (self.indirilen_bayt / 1024) / maks(1, süre), 'domain_stats': self.domain_stats } ``` ### Profilleme Darboğazları belirlemek için kodunuzun profilini oluşturun: ```python import cProfile import pstats import io def profile_function(func, *args, **kwargs): """Bir fonksiyonun profilini oluşturun ve istatistikleri yazdırın.""" pr = cProfile.Profile() pr.enable() result = func(*args, **kwargs) pr.disable() s = io.StringIO() ps = pstats.Stats(pr, stream=s).sort_stats('cumulative') ps.print_stats(20) # Toplam zamana göre ilk 20 fonksiyonu yazdır print(s.getvalue()) return result # Kullanım profile_function(scrape_batch, urls) ``` ### Günlük Kaydı Analiz için ayrıntılı günlük kaydı uygulayın: ```python import logging import time # Günlük kaydını yapılandırın logging.basicConfig( level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s', filename='scraping.log' ) logger = logging.getLogger('scraper') async def scrape_with_logging(url, session): start_time = time.time() logger.info(f"{url} için istek başlatılıyor") try: async with session.get(url) as response: duration = time.time() - start_time size = len(await response.read()) logger.info( f"{url} Tamamlandı - Durum: {response.status}, " f"Boyut: {size} bayt, Zaman: {duration:.2f}sn" ) return await response.text() except Exception as e: duration = time.time() - start_time logger.error(f"Başarısız {url} - Hata: {str(e)}, Süre: {duration:.2f}s") raise ``` ## Özel Teknikler ### JavaScript-Ağır Siteler İçin Başsız Tarayıcılar JavaScript gerektiren siteler için başsız tarayıcılar kullanın: ```python from playwright.async_api import async_playwright async def scrape_js_site(url): async with async_playwright() as p: browser = await p.chromium.launch(headless=True) page = await browser.new_page() # Zaman aşımını ayarla page.set_default_timeout(30000) # Gezin ve ağ boşta kalana kadar bekle await page.goto(url, wait_until='networkidle') # İçeriği çıkar content = await page.content() # Tarayıcıyı kapat await browser.close() return content ``` ### Akıllı Tarama Akıllı taramayı uygula stratejiler: ```python sınıfı PriorityCrawler: def __init__(self): self.visited = set() self.queue = [] # (öncelik, url) tuple'ları def add_url(self, url, priority=0): if url not in self.visited: import heapq heapq.heappush(self.queue, (-priority, url)) # Negatif, max-heap async def crawl(self, session, max_urls=100): results = {} count = 0 import heapq while self.queue and count < max_urls: _, url = heapq.heappop(self.queue) if url in self.visited: continue self.visited.add(url) count += 1 try: html = await self.fetch_url(url, session) results[url] = html # Ayıkla ve yeni bağlantıları önceliklendir new_urls = self.extract_links(html, url) new_url için, öncelik new_urls'de: self.add_url(new_url, priority) except Exception as e: logger.error(f"{url}: {e} taranırken hata oluştu") return results def extract_links(self, html, base_url): # Bağlantıları ayıkla ve alaka düzeyine göre öncelikleri ata # (url, priority) tuple'larının listesini döndür pass ``` ### İçerik Tabanlı Kısıtlama İçerik türüne göre kazıma hızını ayarla: ```python async def adaptive_scrape(url, session): """Kazıma davranışını içerik türüne göre uyarla.""" # Önce session.head(url) ile içerik türünün asenkron olduğunu kontrol etmek için bir HEAD isteği yap as head_response: content_type = head_response.headers.get('Content-Type', '') content_length = int(head_response.headers.get('Content-Length', 0)) # Davranışı içeriğe göre ayarla if 'text/html' in content_type: # Standart HTML sayfası await asyncio.sleep(1) # Standart gecikme elif 'application/json' in content_type: # API uç noktası - daha hızlı olabilir await asyncio.sleep(0.5) elif content_length > 1000000: # Büyük dosya - daha dikkatli olun await asyncio.sleep(5) else: # Varsayılan davranış await asyncio.sleep(2) # Şimdi gerçek isteği session.get(url) ile async yapın response: return await response.text() ``` ## Sonuç Kazıma işlemlerini optimize etmek, performans, kaynak kullanımı ve etik hususlar arasında bir denge kurmaktır. Bu kılavuzdaki teknikleri uygulayarak, hedef web siteleri ve kendi kaynaklarınız üzerindeki etkiyi en aza indirirken verileri etkili bir şekilde toplayan verimli kazıma sistemleri oluşturabilirsiniz. En verimli kazıyıcının şunları yapan olduğunu unutmayın: 1. Yalnızca ihtiyaç duyduğu şeyleri toplar 2. Hedef web sitesinin kaynaklarına saygı gösterir 3. Hesaplama kaynaklarını verimli bir şekilde kullanır 4. Hataları zarif bir şekilde işler 5. Değişen koşullara uyum sağlar Kazıma işlemlerinizi her zaman izleyin ve hedef web sitelerinden gelen performans ölçümlerine ve geri bildirimlere göre yaklaşımınızı ayarlamaya hazır olun. ---Son güncelleme: 2025-01-15