# Panduan Optimalisasi Scraping ## Pendahuluan Scraping web merupakan komponen mendasar dari proyek WhytCard, tetapi dapat menghabiskan banyak sumber daya dan menghadirkan tantangan kinerja. Panduan ini menguraikan strategi dan praktik terbaik untuk mengoptimalkan operasi scraping guna memaksimalkan efisiensi sekaligus meminimalkan penggunaan sumber daya dan dampak pada situs web target. Bahasa Indonesia: ## Daftar Isi 1. [Prinsip Dasar](#fundamental-principles) 2. [Arsitektur Terdistribusi](#distributed-architecture) 3. [Optimasi Permintaan HTTP](#http-request-optimization) 4. [Paralelisasi dan Konkurensi](#parallelization-and-concurrency) 5. [Optimasi Penguraian HTML](#html-parsing-optimization) 6. [Manajemen Sumber Daya](#resource-management) 7. [Pemantauan dan Pembuatan Profil](#monitoring-and-profiling) 8. [Teknik Khusus](#specialized-techniques) ## Prinsip Dasar ### Efisiensi vs. Kesopanan Pengoptimalan pengikisan harus menyeimbangkan dua tujuan yang terkadang bertentangan: 1. **Efisiensi**: Memaksimalkan kecepatan pengumpulan data dan penggunaan sumber daya 2. **Kesopanan**: Meminimalkan dampak pada situs web target dan menghargai sumber daya mereka Selalu utamakan menjadi warga web yang baik dibandingkan kinerja murni ketika tujuan ini bertentangan. ### Metrik Utama Saat mengoptimalkan operasi pengikisan, fokuslah pada metrik utama ini: - **Halaman per menit**: Tingkat pengumpulan halaman - **Penggunaan CPU**: Overhead pemrosesan - **Penggunaan memori**: Konsumsi RAM - **Efisiensi jaringan**: Pemanfaatan bandwidth - **Tingkat kesalahan**: Persentase permintaan yang gagal - **Dampak server target**: Beban yang diberikan pada situs yang dikikis ## Arsitektur Terdistribusi ### Distribusi Tugas Untuk pengikisan skala besar, distribusikan tugas ke beberapa pekerja: ```python # Contoh penggunaan Celery untuk pengikisan terdistribusi dari celery import Celery app = Celery('scraping_tasks', broker='redis://localhost:6379/0') @app.task def scrape_url(url): # Logika pengikisan mengembalikan hasil # Tugas pengiriman urls = ["https://example1.com", "https://example2.com", "https://example3.com"] results = [scrape_url.delay(url) for url in urls] ``` ### Penyeimbangan Beban Terapkan penyeimbangan beban untuk mendistribusikan permintaan di beberapa alamat IP atau instans: ```python class LoadBalancer: def __init__(self, proxies): self.proxies = proxies self.current_index = 0 def get_next_proxy(self): proxy = self.proxies[self.current_index] self.current_index = (self.current_index + 1) % len(self.proxies) return proxy ``` ### Rotasi Proxy Gunakan rotasi proxy untuk menghindari pembatasan kecepatan berbasis IP: ```python async def fetch_with_proxy_rotation(url, proxy_manager, session): max_retries = 3 retry_count = 0 while retry_count < max_retries: proxy = proxy_manager.get_next_proxy() try: async with session.get(url, proxy=proxy, timeout=30) as response: if response.status == 200: return await response.text() elif response.status == 429: # Terlalu Banyak Permintaan # Tandai proxy ini sebagai rate-limited proxy_manager.mark_rate_limited(proxy) retry_count += 1 else: retry_count += 1 except Exception as e: # Tandai proxy ini sebagai gagal proxy_manager.mark_failed(proxy) retry_count += 1 raise Exception(f"Gagal mengambil {url} setelah {max_retries} percobaan") ``` ## Optimasi Permintaan HTTP ### Penggabungan Koneksi Gunakan kembali koneksi HTTP untuk mengurangi overhead: ```python async def scrape_with_connection_pooling(): # Buat satu sesi untuk beberapa permintaan async with aiohttp.ClientSession() as session: task = [] for url in urls: task.append(fetch(url, session)) return await asyncio.gather(*tasks) async Bahasa Indonesia: def fetch(url, session): async with session.get(url) as response: return await response.text() ``` ### Dukungan HTTP/2 Gunakan HTTP/2 saat tersedia untuk mendapatkan manfaat dari multiplexing: ```python import httpx async def fetch_with_http2(): async with httpx.AsyncClient(http2=True) as client: response = await client.get("https://example.com") return response.text ``` ### Permintaan Kompresi mengompresi respons untuk mengurangi penggunaan bandwidth: ```python headers = { 'Accept-Encoding': 'gzip, deflate, br', 'User-Agent': 'WhytCardBot/1.0' } async def fetch_with_compression(url, session): async with session.get(url, headers=headers) as response: return await response.text() ``` ### Optimasi Permintaan Hanya meminta apa yang Anda butuhkan: ```python # Hanya meminta yang diperlukan headers headers = { 'Accept': 'text/html', 'Accept-Language': 'en-US,en;q=0.5', 'Accept-Encoding': 'gzip, deflate, br', 'User-Agent': 'WhytCardBot/1.0' } # Gunakan permintaan HEAD untuk memeriksa sumber daya sebelum GET async def check_before_download(url, session): async with session.head(url) as response: if response.status == 200 and response.headers.get('Content-Type') == 'text/html': return await fetch_full_page(url, session) return None ``` ## Paralelisasi dan Konkurensi ### Pengikisan Asinkron Gunakan pemrograman asinkron untuk menangani beberapa permintaan secara bersamaan: ```python import asyncio import aiohttp async def scrape_all(urls): async with aiohttp.ClientSession() as session: task = [scrape_one(url, session) untuk url di url] kembalikan await asyncio.gather(*tasks, return_exceptions=True) async def scrape_one(url, session): coba: async dengan session.get(url, timeout=30) sebagai respons: jika status respons == 200: html = tunggu respons.teks() kembalikan parse_html(html) yang lain: kembalikan None kecuali Pengecualian sebagai e: logger.error(f"Kesalahan saat mengikis {url}: {e}") kembalikan None ``` ### Batasi Konkurensi Terkendali Batasi konkurensi untuk menghindari sumber daya yang kewalahan: ```python async def scrape_with_semaphore(urls, max_concurrent=10): semaphore = asyncio.Semaphore(max_concurrent) async def _scrape_with_limit(url): async dengan semaphore: kembalikan await scrape_url(url) async dengan Bahasa Indonesia: aiohttp.ClientSession() sebagai sesi: tugas = [_scrape_with_limit(url) untuk url di url] kembalikan await asyncio.gather(*tugas, return_exceptions=True) ``` ### Pembatasan Kecepatan Khusus Domain Terapkan batas kecepatan yang berbeda ke domain yang berbeda: ```python dari urllib.parse impor urlparse impor waktu impor asyncio kelas DomainRateLimiter: def __init__(self): # Domain -> {waktu_permintaan_terakhir, permintaan_per_menit} self.domains = {} self.default_rpm = 30 # Default: 30 permintaan per menit def set_domain_limit(self, domain, rpm): jika domain tidak ada di self.domains: self.domains[domain] = {"waktu_permintaan_terakhir": 0, "rpm": rpm} yang lain: self.domains[domain]["rpm"] = rpm asinkron def wait_if_needed(self, url): domain = urlparse(url).netloc jika domain tidak ada di self.domains: self.domains[domain] = {"last_request_time": 0, "rpm": self.default_rpm} domain_info = self.domains[domain] min_interval = 60.0 / domain_info["rpm"] current_time = time.time() elapsed = current_time - domain_info["last_request_time"] jika elapsed < min_interval: wait_time = min_interval - elapsed await asyncio.sleep(wait_time) self.domains[domain]["last_request_time"] = time.time() ``` ## Optimasi Parsing HTML ### Pemilihan Parser Pilih parser yang paling efisien untuk kebutuhan Anda: ```python from bs4 import BeautifulSoup # lxml jauh lebih cepat daripada html.parser html = response.text soup = BeautifulSoup(html, 'lxml') ``` ### Ekstraksi Bertarget Gunakan pemilih bertarget alih-alih mengurai seluruh dokumen: ```python # Alih-alih mengurai semuanya soup = BeautifulSoup(html, 'lxml') links = soup.find_all('a') # Gunakan pemilih CSS untuk ekstraksi bertarget links = soup.select('div.content a.external') # Atau gunakan metode find yang lebih spesifik content_div = soup.find('div', class_='content') if content_div: links = content_div.find_all('a', class_='external') ``` ### Penguraian Streaming Untuk dokumen besar, gunakan pengurai streaming: ```python from lxml import etree def stream_parse_large_xml(file_path): """Parsing file XML besar tanpa memuatnya sepenuhnya ke dalam memori.""" context = etree.iterparse(file_path, events=('end',), Bahasa Indonesia: tag='item') untuk acara, elem dalam konteks: # Memproses elemen process_element(elem) # Menghapus elemen untuk membebaskan memori elem.clear() # Juga menghilangkan referensi yang sekarang kosong dari simpul akar ke elem sementara elem.getprevious() bukan None: del elem.getparent()[0] del context ``` ### Ekspresi Reguler untuk Kasus Sederhana Untuk ekstraksi yang sangat sederhana, regex bisa lebih cepat: ```python import re def extract_all_emails(text): """Ekstrak semua email dari teks menggunakan regex.""" email_pattern = r'[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\.[a-zA-Z]{2,}' return re.findall(email_pattern, text) ``` ## Manajemen Sumber Daya ### Manajemen Memori Terapkan strategi untuk meminimalkan penggunaan memori: ```python def process_large_dataset(file_path): """Memproses kumpulan data besar dengan penggunaan memori minimal.""" # Gunakan generator alih-alih daftar dengan open(file_path, 'r') as f: for line in f: yield process_line(line) def scrape_with_memory_limit(urls, batch_size=100): """Mengikis URL secara batch untuk membatasi penggunaan memori.""" for i in range(0, len(urls), batch_size): batch = urls[i:i+batch_size] results = scrape_batch(batch) process_and_save_results(results) # Membebaskan hasil memori = None ``` ### Cache Disk Cache respons ke disk untuk menghindari permintaan yang berlebihan: ```python import os import hashlib import pickle class DiskCache: def __init__(self, cache_dir='./cache'): self.cache_dir = cache_dir os.makedirs(cache_dir, exist_ok=True) def _get_cache_path(self, url): """Buat jalur berkas untuk menyimpan konten URL dalam cache.""" url_hash = hashlib.md5(url.encode()).hexdigest() return os.path.join(self.cache_dir, url_hash) def get(self, url): """Ambil konten dari cache jika ada.""" cache_path = self._get_cache_path(url) if os.path.exists(cache_path): with open(cache_path, 'rb') as f: return pickle.load(f) return None def set(self, url, content): """Simpan konten dalam cache.""" cache_path = self._get_cache_path(url) with open(cache_path, 'wb') as f: pickle.dump(content, f) ``` Bahasa Indonesia: ### Pemrosesan Inkremental Memproses data secara inkremental untuk menghindari lonjakan memori: ```python def incremental_scrape_and_process(urls): """Mengikis dan memproses URL secara inkremental.""" for url in urls: html = scrape_url(url) if html: data = extract_data(html) process_data(data) save_data(data) # Membebaskan memori html = None data = None ``` ## Pemantauan dan Pembuatan Profil ### Metrik Kinerja Melacak metrik kinerja utama: ```python import time from dataclasses import dataclass from typing import Dict, List @dataclass class ScrapingMetrics: start_time: float = 0 end_time: float = 0 urls_processed: int = 0 successful_requests: int = 0 failed_requests: int = 0 bytes_downloaded: int = 0 domain_stats: Dict[str, Dict] = None def __post_init__(self): jika self.domain_stats adalah None: self.domain_stats = {} def start(self): self.start_time = time.time() def stop(self): self.end_time = time.time() def add_request(self, url, success, size=0): dari urllib.parse import urlparse domain = urlparse(url).netloc jika domain tidak ada di self.domain_stats: self.domain_stats[domain] = { 'permintaan': 0, 'sukses': 0, 'gagal': 0, 'byte': 0 } self.urls_processed += 1 self.domain_stats[domain]['permintaan'] += 1 jika sukses: self.sukses_permintaan += 1 self.domain_stats[domain]['sukses'] += 1 self.bytes_downloaded += ukuran self.domain_stats[domain]['bytes'] += ukuran else: self.failed_requests += 1 self.domain_stats[domain]['failed'] += 1 def get_summary(self): durasi = self.end_time - self.start_time if self.end_time > 0 else time.time() - self.start_time return { 'duration_seconds': durasi, 'urls_processed': self.urls_processed, 'successful_requests': self.successful_requests, 'failed_requests': self.failed_requests, 'success_rate': self.successful_requests / maks(1, self.urls_processed), 'requests_per_detik': self.urls_processed / maks(1, durasi), 'bytes_downloaded': self.bytes_downloaded, 'download_rate_kbps': (self.bytes_downloaded / 1024) / max(1, duration), 'domain_stats': self.domain_stats } ``` ### Profiling Profil kode Anda untuk mengidentifikasi hambatan: ```python import cProfile import pstats import io def profile_function(func, *args, **kwargs): """Profilkan fungsi dan cetak statistik.""" pr = cProfile.Profile() pr.enable() result = func(*args, **kwargs) pr.disable() s = io.StringIO() ps = pstats.Stats(pr, stream=s).sort_stats('cumulative') ps.print_stats(20) # Cetak 20 fungsi teratas menurut waktu kumulatif print(s.getvalue()) return result # Penggunaan profile_function(scrape_batch, urls) ``` ### Logging Terapkan logging terperinci untuk analisis: ```python Bahasa Indonesia: impor pencatatan impor waktu # Konfigurasikan pencatatan logging.basicConfig( level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s', filename='scraping.log' ) logger = logging.getLogger('scraper') asinkron def scrape_with_logging(url, sesi): start_time = time.time() logger.info(f"Memulai permintaan ke {url}") coba: asinkron dengan sesi.get(url) sebagai respons: durasi = time.time() - waktu_mulai ukuran = len(tunggu respons.baca()) logger.info( f"Selesai {url} - Status: {response.status}, " f"Ukuran: {size} byte, Waktu: {duration:.2f}d" ) kembalikan tunggu respons.teks() kecuali Pengecualian sebagai e: durasi = time.time() - waktu_mulai logger.error(f"Gagal {url} - Galat: {str(e)}, Waktu: {duration:.2f}s") raise ``` ## Teknik Khusus ### Peramban Tanpa Kepala untuk Situs yang Banyak Menggunakan JavaScript Gunakan peramban tanpa kepala untuk situs yang memerlukan JavaScript: ```python from playwright.async_api import async_playwright async def scrape_js_site(url): async with async_playwright() as p: browser = await p.chromium.launch(headless=True) page = await browser.new_page() # Tetapkan batas waktu page.set_default_timeout(30000) # Navigasi dan tunggu jaringan diam await page.goto(url, wait_until='networkidle') # Ekstrak konten content = await page.content() # Tutup peramban await browser.close() return content ``` ### Perayapan Cerdas Terapkan strategi perayapan cerdas: ```python class PriorityCrawler: def __init__(self): self.visited = set() self.queue = [] # (prioritas, url) tuple def add_url(self, url, priority=0): jika url tidak ada di self.visited: import heapq heapq.heappush(self.queue, (-prioritas, url)) # Negatif untuk max-heap async def crawl(self, session, max_urls=100): results = {} count = 0 import heapq while self.queue and count < max_urls: _, url = heapq.heappop(self.queue) jika url ada di self.visited: lanjutkan self.visited.add(url) count += 1 try: html = await self.fetch_url(url, session) results[url] = html # Ekstrak dan prioritaskan tautan baru new_urls = self.extract_links(html, url) untuk new_url, priority di new_urls: self.add_url(new_url, priority) kecuali Exception as e: logger.error(f"Kesalahan saat merayapi {url}: {e}") return results def extract_links(self, html, base_url): # Ekstrak tautan dan tetapkan prioritas berdasarkan relevansi # Kembalikan daftar tupel (url, prioritas) pass ``` ### Pembatasan Berbasis Konten Sesuaikan kecepatan pengikisan berdasarkan jenis konten: ```python async def adaptive_scrape(url, session): """Sesuaikan perilaku pengikisan berdasarkan jenis konten.""" # Pertama buat permintaan HEAD untuk memeriksa jenis konten async dengan session.head(url) sebagai head_response: content_type = head_response.headers.get('Content-Type', '') content_length = int(head_response.headers.get('Content-Length', 0)) # Sesuaikan perilaku berdasarkan konten if 'text/html' in content_type: # Halaman HTML standar await asyncio.sleep(1) # Penundaan standar elif 'application/json' in content_type: # Titik akhir API - dapat lebih cepat await asyncio.sleep(0.5) elif content_length > 1000000: # Berkas besar - lebih berhati-hatilah await asyncio.sleep(5) else: # Perilaku default await asyncio.sleep(2) # Sekarang buat permintaan aktual asyncio dengan session.get(url) sebagai respons: return await response.text() ``` ## Kesimpulan Mengoptimalkan operasi scraping adalah keseimbangan antara kinerja, penggunaan sumber daya, dan pertimbangan etika. Dengan menerapkan teknik dalam panduan ini, Anda dapat membuat sistem scraping efisien yang mengumpulkan data secara efektif sambil meminimalkan dampak pada situs web target dan sumber daya Anda sendiri. Ingatlah bahwa scraper yang paling efisien adalah yang: 1. Hanya mengumpulkan apa yang dibutuhkannya 2. Menghargai sumber daya situs web target 3. Menggunakan sumber daya komputasi secara efisien 4. Menangani kesalahan dengan baik 5. Beradaptasi dengan kondisi yang berubah Selalu pantau operasi scraping Anda dan bersiaplah untuk menyesuaikan pendekatan Anda berdasarkan metrik kinerja dan umpan balik dari situs web target. ---Terakhir diperbarui: 2025-01-15