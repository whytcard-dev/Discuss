# คู่มือการขูดข้อมูลอย่างมีจริยธรรมสำหรับ WhytCard 

## บทนำ 

การขูดข้อมูลบนเว็บเป็นหัวใจสำคัญของโครงการ WhytCard แต่จะต้องดำเนินการในลักษณะที่มีจริยธรรม มีความรับผิดชอบ และถูกต้องตามกฎหมาย คู่มือนี้กำหนดหลักการและแนวทางปฏิบัติที่ต้องปฏิบัติตามเพื่อให้แน่ใจว่ากิจกรรมการขูดข้อมูลทั้งหมดเคารพสิทธิ์ของเจ้าของเว็บไซต์ กฎหมายที่บังคับใช้ และมาตรฐานทางจริยธรรม 
## สารบัญ 

1. [หลักการพื้นฐาน](#หลักการพื้นฐาน) 
2. [แง่มุมทางกฎหมาย](#แง่มุมทางกฎหมาย) 
3. [แนวทางปฏิบัติที่ดีที่สุดทางเทคนิค](#แนวทางปฏิบัติที่ดีที่สุดทางเทคนิค) 
4. [การเคารพทรัพยากร](#การเคารพทรัพยากร) 
5. [การปกป้องข้อมูลส่วนบุคคล](#การปกป้องข้อมูลส่วนบุคคล) 
6. [การจัดทำเอกสารและความโปร่งใส](#การจัดทำเอกสารและความโปร่งใส) 
7. [กรณีพิเศษ](#กรณีพิเศษ) 
8. [รายการตรวจสอบการขูดข้อมูลตามจริยธรรม](#รายการตรวจสอบการขูดข้อมูลตามจริยธรรม) 

## หลักการพื้นฐาน 

### ปรัชญาการขูดข้อมูลตามจริยธรรม 

การขูดข้อมูลตามจริยธรรมมีพื้นฐานอยู่บนหลักการพื้นฐานสามประการ: 

1. **ความเคารพ**: เคารพเจ้าของเว็บไซต์ ข้อกำหนดในการใช้งาน และ ทรัพยากร 
2. **ความสมส่วน**: ดึงเฉพาะข้อมูลที่จำเป็นด้วยผลกระทบน้อยที่สุด 
3. **ความโปร่งใส**: เปิดเผยตัวตนของบอทและความตั้งใจในการขูดข้อมูล 

### ค่านิยมของ WhytCard เกี่ยวกับการขูดข้อมูล 

ในฐานะโครงการ WhytCard เรามุ่งมั่นที่จะ: 

- ไม่ทำร้ายเว็บไซต์ที่เราขูดข้อมูล 
- เคารพกฎที่ชัดเจนและโดยนัยของเว็บไซต์อย่างเคร่งครัด 
- โปร่งใสเกี่ยวกับตัวตนและวัตถุประสงค์ของเรา 
- ใช้ข้อมูลอย่างมีความรับผิดชอบและสอดคล้องกับภารกิจของเรา 
- ให้ความสำคัญกับ API อย่างเป็นทางการเมื่อมี 

## แง่มุมทางกฎหมาย 

### กรอบกฎหมายทั่วไป 

การขูดข้อมูลบนเว็บอยู่ภายใต้กรอบกฎหมายหลายฉบับซึ่งแตกต่างกันไปในแต่ละประเทศ: 

- **ลิขสิทธิ์**: เนื้อหาของเว็บไซต์โดยทั่วไปได้รับการคุ้มครองโดยลิขสิทธิ์ 
- **เงื่อนไขการใช้งาน**: ข้อกำหนดในการให้บริการของเว็บไซต์อาจห้ามการขูดข้อมูลโดยชัดเจน 
- **การคุ้มครองข้อมูล**: กฎหมายเช่น GDPR ในยุโรปปกป้องข้อมูลส่วนบุคคล 
- **การเข้าถึงโดยไม่ได้รับอนุญาต**: เขตอำนาจศาลบางแห่งทำให้การกระทำผิดกฎหมาย การเข้าถึงระบบคอมพิวเตอร์โดยไม่ได้รับอนุญาต 

### คดีตัวอย่างที่สำคัญ 

คำตัดสินของศาลที่สำคัญบางส่วนเกี่ยวกับการขูดข้อมูล: 

- **hiQ Labs v. LinkedIn** (สหรัฐอเมริกา): กำหนดให้การขูดข้อมูลสาธารณะไม่จำเป็นต้องผิดกฎหมาย 
- **Ryanair v. PR Aviation** (สหภาพยุโรป): ยืนยันว่าเงื่อนไขการใช้งานสามารถจำกัดการขูดข้อมูลได้ตามสัญญา 
- **QVC v. Resultly** (สหรัฐอเมริกา): เน้นย้ำถึงความสำคัญของการไม่ทำให้เซิร์ฟเวอร์โหลดเกิน 

### การปฏิบัติตามกฎหมายสำหรับ WhytCard 

เพื่อให้เป็นไปตามกฎหมาย: 

1. **ตรวจสอบข้อกำหนดในการให้บริการเสมอ** ก่อนทำการขูดข้อมูลเว็บไซต์ 
2. **เคารพแท็ก "noindex" และ "nofollow"** ในเมตาแท็ก 
3. **อย่าหลีกเลี่ยงมาตรการป้องกันทางเทคนิค** (CAPTCHA, ข้อจำกัดการเข้าถึง) 
4. **บันทึกแนวทางปฏิบัติของคุณ** เพื่อแสดงให้เห็นถึงความซื่อสัตย์สุจริต 
5. **ปรึกษาหารือกับทนายความ** หากมีข้อสงสัยเกี่ยวกับความถูกต้องตามกฎหมายของการขูดข้อมูล 

## แนวทางปฏิบัติที่ดีที่สุดทางเทคนิค 

### การเคารพ robots.txt 

ไฟล์ robots.txt กำหนดกฎการเข้าถึงสำหรับหุ่นยนต์: 

```python 
จาก urllib.robotparser นำเข้า RobotFileParser 
จาก urllib.parse นำเข้า urlparse 

def is_allowed(url, user_agent="WhytCardBot/1.0"): 
"""ตรวจสอบว่าสามารถขูด URL ตาม robots.txt ได้หรือไม่""" 
rp = RobotFileParser() 
rp.set_url(f"{urlparse(url).scheme}://{urlparse(url).netloc}/robots.txt") 
rp.read() 
return rp.can_fetch(user_agent, url) 
``` 

### การระบุที่ถูกต้อง 

ใช้ User-Agent ที่ระบุบอทของคุณอย่างชัดเจนเสมอ: 

```python 
headers = { 
'User-Agent': 'WhytCardBot/1.0 (+https://whytcard.com/bot; bot@whytcard.com)', 
# ส่วนหัวอื่นๆ... 
} 
``` 

### ความล่าช้าของคำขอ 

ใช้ความล่าช้าที่เหมาะสมระหว่างคำขอ: 

```python 
import time 
import random 

def gentle_request(url, session, min_delay=1, max_delay=3): 
"""ส่งคำขอโดยมีการหน่วงเวลาอย่างสุภาพระหว่างคำขอ""" 
# รอการหน่วงเวลาแบบสุ่ม 
delay = random.uniform(min_delay, max_delay) 
time.sleep(delay) 

# ส่งคำขอ 
response = session.get(url, headers=headers) 
return response 
``` 

### การจัดการข้อผิดพลาด 

เคารพรหัสข้อผิดพลาด HTTP และปรับเปลี่ยนพฤติกรรมของคุณ ตามนั้น: 

```python 
async def respectful_fetch(url, session): 
"""ดึง URL ออกมาในลักษณะที่เคารพซึ่งกันและกัน""" 
ลอง: 

async กับ session.get(url, headers=headers) เป็นคำตอบ: 
if response.status == 200: 
return await response.text() 
elif response.status == 429: # คำขอมากเกินไป 
# รอสักครู่ก่อนลองใหม่อีกครั้ง 

wait_time = int(response.headers.get('Retry-After', 60)) 
logger.info(f"Rate limited, waiting {wait_time} seconds") 
await asyncio.sleep(wait_time) 
return await respectful_fetch(url, session) 
elif response.status in (403, 404): 
# อย่าลองใหม่อีกครั้ง ข้อผิดพลาด 403/404 
logger.warning(f"Access denied or not found: {url}") 
return None 
else: 
# รอและลองใหม่อีกครั้งสำหรับข้อผิดพลาดอื่นๆ 
logger.warning(f"Error {response.status} for {url}, retrying in 5s") 
await asyncio.sleep(5) 
return await respectful_fetch(url, session) 
except Exception as e: 
logger.error(f"Exception while fetching {url}: {str(e)}") 
return None 
``` 

## การเคารพทรัพยากร 

### การจำกัดอัตรา 

ปรับอัตราคำขอของคุณให้เหมาะกับขนาดและทรัพยากรของไซต์เป้าหมาย: 

- **ไซต์เชิงพาณิชย์ขนาดใหญ่**: 1 คำขอทุก 1-3 วินาที 
- **ไซต์ขนาดกลาง**: 1 คำขอทุก 3-10 วินาที 
- **ไซต์ขนาดเล็ก**: 1 คำขอทุก 10-60 วินาทีขึ้นไป 

### ระยะเวลาการขูดข้อมูล

เลือกช่วงที่มีปริมาณการใช้งานต่ำสำหรับการดำเนินการที่เข้มข้น:

- **ชั่วโมงนอกชั่วโมงเร่งด่วน**: เลือกช่วงกลางคืนหรือสุดสัปดาห์

- **หลีกเลี่ยงช่วงพีค**: อย่าขูดข้อมูลในช่วงเวลาพีคที่ทราบ

- **ปรับตัว**: ลดอัตราของคุณหากตรวจพบว่ามีการชะลอตัว

### การลดผลกระทบ

เทคนิคในการลดผลกระทบต่อเซิร์ฟเวอร์เป้าหมาย:

1. **แคชอัจฉริยะ**: อย่าดึงข้อมูลหน้าเดียวกันซ้ำหลายครั้ง

2. **การเลือกสรร**: ดึงข้อมูลเฉพาะหน้าที่คุณต้องการจริงๆ

3. **การบีบอัด**: ขอการตอบกลับแบบบีบอัดเพื่อลดแบนด์วิดท์

4. **การแบ่งหน้าอย่างมีประสิทธิภาพ**: เคารพโครงสร้างการแบ่งหน้าของไซต์

## การปกป้องข้อมูลส่วนบุคคล

### การระบุข้อมูลส่วนบุคคล

ระวังประเภทของข้อมูลที่คุณรวบรวม:

- **ข้อมูลระบุตัวตนโดยตรง**: ชื่อ อีเมล โทรศัพท์ ที่อยู่

- **ข้อมูลระบุตัวตนโดยอ้อม**: ผู้ใช้ ID, นามแฝง 
- **ข้อมูลละเอียดอ่อน**: ความคิดเห็นทางการเมือง สุขภาพ รสนิยมทางเพศ 

### หลักการ GDPR ที่ต้องเคารพ 

หากคุณดำเนินการในยุโรปหรือรวบรวมข้อมูลจากชาวยุโรป: 

1. **การลดขนาด**: รวบรวมข้อมูลที่จำเป็นอย่างเคร่งครัดเท่านั้น 
2. **วัตถุประสงค์**: ใช้ข้อมูลเพื่อจุดประสงค์ที่ตั้งใจเท่านั้น 
3. **การเก็บรักษาที่จำกัด**: ลบข้อมูลเมื่อไม่จำเป็นอีกต่อไป 
4. **ความปลอดภัย**: ปกป้องข้อมูลที่รวบรวมจากการเข้าถึงโดยไม่ได้รับอนุญาต 

### การทำให้ข้อมูลไม่ระบุตัวตน 

เทคนิคในการทำให้ข้อมูลส่วนบุคคลไม่ระบุตัวตน: 

```python 
import hashlib 
import re 

def anonymize_email(email): 
"""ทำให้ที่อยู่อีเมลไม่ระบุตัวตน""" 
if not email: 
return None 

# แฮชที่อยู่อีเมล 
hashed = hashlib.sha256(email.encode()).hexdigest()[:10] 
domain = email.split('@')[-1] 

return f"anon_{hashed}@{domain}" 

def anonymize_phone(phone): 
"""ทำให้หมายเลขโทรศัพท์ไม่ระบุตัวตน""" 
if not phone: 
return None 

# เก็บเฉพาะตัวเลข 
digits = re.sub(r'\D', '', phone) 

# ปิดบังตัวเลขทั้งหมด ยกเว้น 2 ตัวสุดท้าย 
if len(digits) > 2: 
return "X" * (len(digits) - 2) + digits[-2:] 
return "X" * len(digits) 
``` 

## การจัดทำเอกสารและความโปร่งใส 

### การจัดทำเอกสารกิจกรรมการขูดข้อมูล 

จัดทำเอกสารกิจกรรมการขูดข้อมูลเสมอ: 

- **จุดประสงค์**: เหตุใดจึงต้องรวบรวมข้อมูลนี้ 
- **วิธีการ**: รวบรวมข้อมูลนี้อย่างไร 
- **การจัดเก็บ**: จัดเก็บที่ไหนและอย่างไร 
- **การใช้งาน**: จะใช้ข้อมูลนี้อย่างไร 
- **การลบ**: จะลบข้อมูลเมื่อใด 

### ติดต่อและยกเลิก 

ให้ช่องทางการติดต่อกับคุณเสมอ: 

1. **หน้าข้อมูล**: สร้างหน้าเฉพาะเพื่ออธิบายเกี่ยวกับบอตของคุณ (เช่น whytcard.com/bot) 
2. **อีเมลติดต่อ**: ระบุที่อยู่อีเมลใน User-Agent ของคุณ 
3. **กลไกการยกเลิก**: อนุญาตให้ไซต์ร้องขอการยกเว้น 

### การบันทึกกิจกรรม 

รักษาบันทึกโดยละเอียดของกิจกรรมการขูดข้อมูลของคุณ: 

```python 
import logging 
from datetime import datetime 

# การกำหนดค่าตัวบันทึก 
logging.basicConfig( 
filename=f"scraping_log_{datetime.now().strftime('%Y%m%d')}.log", 
level=logging.INFO, 
format='%(asctime)s - %(levelname)s - %(message)s' 
) 

def log_scraping_activity(url, success, data_points=0): 
"""บันทึกกิจกรรมการขูดข้อมูล""" 
logging.info(f"URL: {url}, Success: {success}, Data points: {data_points}") 
``` 

## กรณีพิเศษ 

### API เทียบกับการขูดข้อมูล 

ลำดับความสำคัญในการรวบรวมข้อมูล: 

1. **API อย่างเป็นทางการ**: ให้ความสำคัญกับ API อย่างเป็นทางการเสมอเมื่อมีอยู่เสมอ 
2. **ฟีดข้อมูลสาธารณะ**: ใช้ฟีด RSS, XML หรือ JSON หากมี 
3. **การขูดข้อมูล**: ใช้การขูดข้อมูลเป็นทางเลือกสุดท้ายเท่านั้น 

### ไซต์ที่มีการรับรองความถูกต้อง 

สำหรับไซต์ที่ต้องมีการรับรองความถูกต้อง: 

- **การอนุญาตอย่างชัดเจน**: ขอรับการอนุญาตเป็นลายลักษณ์อักษรจากไซต์ 
- **การเคารพข้อกำหนดในการให้บริการ**: ให้แน่ใจว่าข้อกำหนดในการให้บริการอนุญาตให้ใช้โดยอัตโนมัติ 
- **ข้อจำกัด**: เคารพข้อจำกัดการใช้งานอย่างเคร่งครัด 

### เนื้อหาไดนามิก (JavaScript) 

สำหรับไซต์ที่ใช้ JavaScript จำนวนมาก: 

```python 
จาก playwright.async_api นำเข้า async_playwright 

async def scrape_dynamic_content(url): 
"""ขูดเนื้อหาที่สร้างโดย JavaScript""" 
async ด้วย async_playwright() เป็น p: 
browser = await p.chromium.launch(headless=True) 
page = await browser.new_page() 

# กำหนดค่า User-Agent 
await page.set_extra_http_headers({ 
'User-Agent': 'WhytCardBot/1.0 (+https://whytcard.com/bot)' 
}) 

# โหลดหน้าและรอให้เครือข่ายไม่ได้ใช้งาน 
await page.goto(url) 
await page.wait_for_load_state('networkidle') 

# แยกเนื้อหา 
content = await page.content() 

await browser.close() 
return content 
``` 

## รายการตรวจสอบการขูดข้อมูลอย่างมีจริยธรรม 

ก่อนโครงการขูดข้อมูลแต่ละโครงการ ให้ตรวจสอบจุดต่อไปนี้: 

### การเตรียมการ 
- [ ] ตรวจสอบข้อกำหนดการให้บริการของไซต์เป้าหมาย 
- [ ] ตรวจสอบไฟล์ robots.txt 
- [ ] ค้นหา API หรือทางเลือกอื่นในการขูดข้อมูล 
- [ ] คำจำกัดความที่ชัดเจนของข้อมูลที่จำเป็น 
- [ ] เอกสารประกอบจุดประสงค์ในการขูดข้อมูล 

### การกำหนดค่าทางเทคนิค 
- [ ] ตัวแทนผู้ใช้ที่ระบุตัวตนได้และโปร่งใส 
- [ ] กลไกการจำกัดอัตรา 
- [ ] ระบบแคชเพื่อหลีกเลี่ยงคำขอซ้ำซ้อน 
- [ ] การจัดการข้อผิดพลาดและรหัส HTTP อย่างเหมาะสม 
- [ ] การบันทึกกิจกรรม 

### การดำเนินการ 
- [ ] การตรวจสอบประสิทธิภาพของไซต์เป้าหมาย 
- [ ] การปรับอัตราแบบไดนามิกหากจำเป็น 
- [ ] เคารพต่อข้อบ่งชี้ของเซิร์ฟเวอร์ (429, ลองใหม่อีกครั้งหลังจากนี้) 
- [ ] หยุดทันทีหากตรวจพบปัญหา 

### การประมวลผลภายหลัง 
- [ ] การทำให้ข้อมูลส่วนบุคคลไม่ระบุตัวตน 
- [ ] การจัดเก็บข้อมูลที่ปลอดภัย 
- [ ] การเก็บรักษาแบบจำกัดเวลา 
- [ ] การจัดทำเอกสารข้อมูลที่รวบรวม 

## ข้อสรุป 

การขูดข้อมูลตามจริยธรรมเป็นการรักษาสมดุลระหว่างการเข้าถึงข้อมูลและการเคารพสิทธิ์และทรัพยากรของเจ้าของเว็บไซต์ โดยปฏิบัติตามหลักการและแนวทางปฏิบัติเหล่านี้ โปรเจ็กต์ WhytCard สามารถรวบรวมข้อมูลที่จำเป็นได้ในขณะที่รักษาแนวทางที่รับผิดชอบและเคารพซึ่งกันและกัน 

โปรดจำไว้ว่าจริยธรรมในการขูดข้อมูลไม่ใช่แค่เรื่องของการปฏิบัติตามกฎหมายเท่านั้น แต่ยังรวมถึงความรับผิดชอบต่อระบบนิเวศของเว็บโดยรวมด้วย การขูดข้อมูลตามจริยธรรมช่วยให้เว็บเปิดกว้างและยั่งยืนมากขึ้นสำหรับทุกคน 

--- 

ปรับปรุงล่าสุด: 2025-01-15