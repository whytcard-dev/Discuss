# Руководство по этичному парсингу для WhytCard 

## Введение 

Парсинг веб-сайтов лежит в основе проекта WhytCard, но он должен проводиться этичным, ответственным и законным образом. В этом руководстве определены принципы и методы, которым необходимо следовать, чтобы гарантировать, что все действия по парсингу уважают права владельцев веб-сайтов, применимые законы и этические стандарты. 

## Содержание 

1. [Основные принципы](#fundamental-principles) 
2. [Юридические аспекты](#legal-aspects) 
3. [Технические передовые практики](#technical-best-practices) 
4. [Уважение к ресурсам](#resource-respect) 
5. [Защита персональных данных](#personal-data-protection) 
6. [Документация и прозрачность](#documentation-and-transparency) 
7. [Особые случаи](#special-cases) 
8. [Контрольный список этического парсинга](#ethical-scraping-checklist) 

## Основные принципы 

### Философия этического парсинга 

Этический парсинг основан на трех основных принципах: 

1. **Уважение**: уважайте владельцев веб-сайтов, их условия использования и их ресурсы 
2. **Пропорциональность**: извлекайте только необходимые данные с минимальным воздействием 
3. **Прозрачность**: будьте прозрачны относительно личности бота и намерений парсинга 

### Ценности WhytCard относительно парсинга 

Как проект WhytCard, мы обязуемся: 

- Никогда не причинять вреда веб-сайтам, которые мы парсим 
- Строго соблюдать явные и неявные правила веб-сайтов 
- Быть прозрачными относительно нашей личности и целей 
- Использовать данные ответственно и в соответствии с нашей миссией 
- Отдавать приоритет официальным API, если они доступны 

## Правовые аспекты 

### Общая правовая база 

Веб-парсинг регулируется несколькими правовыми базами, которые различаются в зависимости от страны: 

- **Авторское право**: Контент веб-сайта, как правило, защищен авторским правом 
- **Условия использования**: Условия использования веб-сайта могут явно запрещать парсинг 
- **Защита данных**: Такие законы, как GDPR в Европе, защищают персональные данные 
- **Несанкционированный доступ**: некоторые юрисдикции криминализируют несанкционированный доступ к компьютерным системам 

### Знаменитая судебная практика 

Некоторые важные судебные решения относительно парсинга: 

- **hiQ Labs против LinkedIn** (США): установлено, что парсинг общедоступных данных не обязательно является незаконным 
- **Ryanair против PR Aviation** (ЕС): подтверждено, что условия использования могут договорно ограничивать парсинг 
- **QVC против Resultly** (США): подчеркнута важность недопущения перегрузки серверов 

### Соблюдение правовых норм для WhytCard 

Чтобы оставаться законным: 

1. **Всегда проверяйте ToS** перед парсингом сайта 
2. **Уважайте теги "noindex" и "nofollow"** в метатегах 
3. **Никогда не обходите технические меры защиты** (CAPTCHA, ограничения доступа) 
4. **Документируйте свои действия**, чтобы продемонстрировать добросовестность 
5. **Проконсультируйтесь с юристом**, если сомневаетесь в законности операции по извлечению данных 

## Технические рекомендации 

### Соблюдение robots.txt 

Файл robots.txt определяет правила доступа для роботов: 

```python 
from urllib.robotparser import RobotFileParser 
from urllib.parse import urlparse 

def is_allowed(url, user_agent="WhytCardBot/1.0"): 
"""Проверяет, можно ли извлечь URL-адрес в соответствии с robots.txt.""" 
rp = RobotFileParser() 
rp.set_url(f"{urlparse(url).scheme}://{urlparse(url).netloc}/robots.txt") 
rp.read() 
return rp.can_fetch(user_agent, url) 
``` 

### Правильная идентификация 

Всегда используйте User-Agent, который четко идентифицирует вашего бота: 

```python 
headers = { 
'User-Agent': 'WhytCardBot/1.0 (+https://whytcard.com/bot; bot@whytcard.com)', 
# Другие заголовки... 
} 
``` 

### Задержки запросов 

Внедряйте разумные задержки между запросами: 

```python 
import time 
import random 

def polite_request(url, session, min_delay=1, max_delay=3): 
"""Выполняет запрос с вежливой задержкой между запросами.""" 
# Ожидание случайной задержки 
delay = random.uniform(min_delay, max_delay) 
time.sleep(delay) 

# Выполните запрос 
response = session.get(url, headers=headers) 
return response 
``` 

### Обработка ошибок 

Уважайте коды ошибок HTTP и соответствующим образом адаптируйте свое поведение: 

```python 
async def respectful_fetch(url, session): 
"""Извлекает URL в уважительной манере.""" 
try: 
async with session.get(url, headers=headers) as response: 
if response.status == 200: 
return await response.text() 
elif response.status == 429: # Слишком много запросов 
# Подождите дольше перед повторной попыткой 
wait_time = int(response.headers.get('Retry-After', 60)) 
logger.info(f"Rate limited, waiting {wait_time} seconds") 
await asyncio.sleep(wait_time) 
return await respectful_fetch(url, session) 
elif response.status in (403, 404): 
# Не повторяйте ошибки 403/404 
logger.warning(f"Доступ запрещен или не найден: {url}") 
return None 
else: 
# Подождите и повторите для других ошибок 
logger.warning(f"Ошибка {response.status} для {url}, повтор через 5 с") 
await asyncio.sleep(5) 
return await respectful_fetch(url, session) 
except Exception as e: 
logger.error(f"Исключение при извлечении {url}: {str(e)}") 
return None 
``` 

## Уважение к ресурсам 

### Ограничение скорости 

Адаптируйте скорость запросов к размеру и ресурсам Целевой сайт: 

- **Крупные коммерческие сайты**: 1 запрос каждые 1-3 секунды 
- **Средние сайты**: 1 запрос каждые 3-10 секунд 
- **Небольшие сайты**: 1 запрос каждые 10-60 секунд или чаще 

### Периоды сбора данных 

Отдавайте предпочтение периодам с низким трафиком для интенсивных операций: 

- **Непиковые часы**: Предпочитайте ночи или выходные 
- **Избегайте пиков**: Не выполняйте сбор данных в известные пиковые периоды 
- **Будьте адаптивны**: Снизьте свою скорость, если обнаружите замедления 

### Минимизация воздействия 

Методы снижения воздействия на целевые серверы: 

1. **Умное кэширование**: Не извлекайте одну и ту же страницу несколько раз 
2. **Избирательность**: Извлекайте только те страницы, которые вам действительно нужны 
3. **Сжатие**: Запрашивайте сжатые ответы для снижения пропускной способности 
4. **Эффективная навигация по страницам**: соблюдайте структуру навигаций сайта 

## Защита персональных данных 

### Идентификация персональных данных 

Будьте бдительны в отношении типов данных, которые вы собираете: 

- **Прямые идентификационные данные**: имена, адреса электронной почты, телефоны, адреса 
- **Косвенные идентификационные данные**: идентификаторы пользователей, псевдонимы 
- **Конфиденциальные данные**: политические взгляды, здоровье, сексуальная ориентация 

### Принципы GDPR, которые следует соблюдать 

Если вы работаете в Европе или собираете данные от европейцев: 

1. **Минимизация**: собирайте только строго необходимые данные 
2. **Цель**: используйте данные только по назначению 
3. **Ограниченное хранение**: удаляйте данные, когда они больше не нужны 
4. **Безопасность**: защищайте собранные данные от несанкционированного доступа 

### Анонимизация данных 

Методы анонимизации персональных data: 

```python 
import hashlib 
import re 

def anonymize_email(email): 
"""Анонимизирует адрес электронной почты.""" 
if not email: 
return None 

# Хеширует адрес электронной почты 
hashed = hashlib.sha256(email.encode()).hexdigest()[:10] 
domain = email.split('@')[-1] 

return f"anon_{hashed}@{domain}" 

def anonymize_phone(phone): 
"""Анонимизирует номер телефона.""" 
if not phone: 
return None 

# Оставить только цифры 
digits = re.sub(r'\D', '', phone) 

# Маскирует все цифры, кроме последних 2 
if len(digits) > 2: 
return "X" * (len(digits) - 2) + digits[-2:] 
return "X" * len(digits) 
``` 

## Документирование и прозрачность 

### Документирование действий по извлечению данных 

Всегда документируйте свои действия по извлечению данных: 

- **Цель**: Зачем собираются эти данные? 
- **Метод**: Как они собираются? 
- **Хранение**: Где и как они хранятся? 
- **Использование**: Как они будут использоваться? 
- **Удаление**: Когда они будут удалены? 

### Контакты и отказ 

Всегда указывайте способ связаться с вами: 

1. **Информационная страница**: создайте специальную страницу с описанием вашего бота (например, whytcard.com/bot) 
2. **Контактный адрес электронной почты**: укажите адрес электронной почты в вашем User-Agent 
3. **Механизм отказа**: разрешите сайтам запрашивать исключение 

### Ведение журнала активности 

Ведите подробные журналы ваших действий по скрапингу: 

```python 
import logging 
from datetime import datetime 

# Конфигурация логгера 
logging.basicConfig( 
filename=f"scraping_log_{datetime.now().strftime('%Y%m%d')}.log", 
level=logging.INFO, 
format='%(asctime)s - %(levelname)s - %(message)s' 
) 

def log_scraping_activity(url, success, data_points=0): 
"""Регистрирует активность парсинга.""" 
logging.info(f"URL: {url}, Success: {success}, Data points: {data_points}") 
``` 

## Особые случаи 

### API против парсинга 

Приоритетный порядок сбора данных: 

1. **Официальные API**: всегда отдавайте приоритет официальным API, если они существуют 
2. **Публичные каналы данных**: используйте каналы RSS, XML или JSON, если они доступны 
3. **Парсинг**: используйте парсинг только в крайнем случае 

### Сайты с аутентификацией 

Для сайтов, требующих аутентификации: 

- **Явная авторизация**: получите письменное разрешение от сайта 
- **Соблюдение ToS**: убедитесь, что ToS разрешает автоматическое использование 
- **Ограничения**: Строго соблюдайте ограничения использования 

### Динамический контент (JavaScript) 

Для сайтов, использующих много JavaScript: 

```python 
from playwright.async_api import async_playwright 

async def scrape_dynamic_content(url): 
"""Извлечение контента, созданного JavaScript.""" 
async with async_playwright() as p: 
browser = await p.chromium.launch(headless=True) 
page = await browser.new_page() 

# Настройте User-Agent 
await page.set_extra_http_headers({ 
'User-Agent': 'WhytCardBot/1.0 (+https://whytcard.com/bot)' 
}) 

# Загрузите страницу и дождитесь простоя сети 
await page.goto(url) 
await page.wait_for_load_state('networkidle') 

# Извлечение контента 
content = await page.content() 

await browser.close() 
return content 
``` 

## Контрольный список этичного парсинга 

Перед каждым проектом парсинга проверьте следующие пункты: 

### Подготовка 
- [ ] Проверка ToS целевого сайта 
- [ ] Проверка файла robots.txt 
- [ ] Поиск API или альтернатив парсингу 
- [ ] Четкое определение необходимых данных 
- [ ] Документация цели парсинга 

### Техническая конфигурация 
- [ ] Идентифицируемый и прозрачный User-Agent 
- [ ] Механизм ограничения скорости 
- [ ] Система кэширования для предотвращения избыточных запросов 
- [ ] Соответствующая обработка ошибок и кодов HTTP 
- [ ] Регистрация активности 

### Выполнение 
- [ ] Мониторинг производительности целевого сайта 
- [ ] Динамическая корректировка скорости при необходимости 
- [ ] Уважение к указаниям сервера (429, Retry-After) 
- [ ] Немедленная остановка при обнаружении проблемы 

### Постобработка 
- [ ] Анонимизация персональных данных 
- [ ] Безопасное хранение данных 
- [ ] Ограниченное по времени хранение 
- [ ] Документирование собранных данных 

## Заключение 

Этический парсинг — это баланс между доступом к данным и уважением прав и ресурсов владельцев веб-сайтов. Следуя этим принципам и практикам, проект WhytCard может собирать необходимые данные, сохраняя при этом ответственный и уважительный подход. 

Помните, что этика парсинга — это не только вопрос соблюдения законодательства, но и ответственности перед экосистемой сети в целом. Уважительный парсинг способствует более открытому и устойчивому Интернету для всех. 

--- 

Последнее обновление: 2025-01-15