# Guia de Otimização de Scraping ## Introdução O web scraping é um componente fundamental do projeto WhytCard, mas pode consumir muitos recursos e apresentar desafios de desempenho. Este guia descreve estratégias e práticas recomendadas para otimizar as operações de scraping, maximizando a eficiência e minimizando o uso de recursos e o impacto nos sites de destino. ## Índice 1. [Princípios Fundamentais](#fundamental-principles) 2. [Arquitetura Distribuída](#distributed-architecture) 3. [Otimização de Solicitações HTTP](#http-request-optimization) 4. [Paralelismo e Simultaneidade](#parallelization-and-concurrency) 5. [Otimização de Análise Sintética de HTML](#html-parsing-optimization) 6. [Gerenciamento de Recursos](#resource-management) 7. [Monitoramento e Criação de Perfil](#monitoring-and-profiling) 8. [Técnicas Especializadas](#specialized-techniques) ## Princípios Fundamentais ### Eficiência vs. Polidez A otimização de scraping deve equilibrar dois objetivos, às vezes conflitantes: 1. **Eficiência**: Maximizar a velocidade da coleta de dados e a utilização de recursos 2. **Polidez**: Minimizar o impacto nos sites de destino e respeitar seus recursos Sempre priorize ser um bom cidadão da web em detrimento do desempenho puro quando esses objetivos entrarem em conflito. ### Métricas principais Ao otimizar as operações de scraping, concentre-se nestas métricas principais: - **Páginas por minuto**: Taxa de coleta de páginas - **Uso da CPU**: Sobrecarga de processamento - **Uso de memória**: Consumo de RAM - **Eficiência da rede**: Utilização da largura de banda - **Taxa de erros**: Porcentagem de solicitações com falha - **Impacto no servidor de destino**: Carga colocada em sites scraping ## Arquitetura distribuída ### Distribuição de tarefas Para scraping em larga escala, distribua as tarefas entre vários trabalhadores: ```python # Exemplo usando Celery para scraping distribuído from celery import Celery app = Celery('scraping_tasks', broker='redis://localhost:6379/0') @app.task def scrape_url(url): # Resultado de retorno da lógica de scraping # Despachar tarefas urls = ["https://example1.com", "https://example2.com", "https://example3.com"] results = [scrape_url.delay(url) para url em urls] ``` ### Balanceamento de carga Implemente o balanceamento de carga para distribuir solicitações entre vários endereços IP ou instâncias: ```python class LoadBalancer: def __init__(self, proxies): self.proxies = proxies self.current_index = 0 def get_next_proxy(self): proxy = self.proxies[self.current_index] self.current_index = (self.current_index + 1) % len(self.proxies) return proxy ``` ### Rotação de proxy Use a rotação de proxy para evitar limitação de taxa baseada em IP: ```python async def fetch_with_proxy_rotation(url, proxy_manager, session): max_retries = 3 retry_count = 0 while retry_count < max_retries: proxy = proxy_manager.get_next_proxy() try: async com session.get(url, proxy=proxy, timeout=30) como resposta: if response.status == 200: return await response.text() elif response.status == 429: # Muitas solicitações # Marque este proxy como com taxa limitada proxy_manager.mark_rate_limited(proxy) retry_count += 1 else: retry_count += 1 except Exception as e: # Marque este proxy como com falha proxy_manager.mark_failed(proxy) retry_count += 1 raise Exception(f"Falha ao buscar {url} após {max_retries} tentativas") ``` ## Otimização de solicitação HTTP ### Pool de conexões Reutilize conexões HTTP para reduzir a sobrecarga: ```python async def scrape_with_connection_pooling(): # Crie uma única sessão para várias solicitações async com aiohttp.ClientSession() como sessão: tasks = [] para url em urls: tasks.append(fetch(url, session)) return await asyncio.gather(*tasks) async def fetch(url, session): async with session.get(url) as response: return await response.text() ``` ### Suporte HTTP/2 Use HTTP/2 quando disponível para se beneficiar da multiplexação: ```python import httpx async def fetch_with_http2(): async with httpx.AsyncClient(http2=True) as client: response = await client.get("https://example.com") return response.text ``` ### Compressão Solicite respostas compactadas para reduzir o uso de largura de banda: ```python headers = { 'Accept-Encoding': 'gzip, deflate, br', 'User-Agent': 'WhytCardBot/1.0' } async def fetch_with_compression(url, session): async with session.get(url, headers=headers) como resposta: return await response.text() ``` ### Otimização de solicitação Solicite apenas o que você precisa: ```python # Solicite apenas os cabeçalhos necessários headers = { 'Accept': 'text/html', 'Accept-Language': 'en-US,en;q=0.5', 'Accept-Encoding': 'gzip, deflate, br', 'User-Agent': 'WhytCardBot/1.0' } # Use solicitações HEAD para verificar recursos antes de GET async def check_before_download(url, session): async com session.head(url) como resposta: if response.status == 200 and response.headers.get('Content-Type') == 'text/html': return await fetch_full_page(url, session) return None ``` ## Paralelização e simultaneidade ### Scraping assíncrono Use programação assíncrona para manipular várias solicitações simultaneamente: ```python import asyncio import aiohttp async def scrape_all(urls): async with aiohttp.ClientSession() as session: tasks = [scrape_one(url, session) for url in urls] return await asyncio.gather(*tasks, return_exceptions=True) async def scrape_one(url, session): try: async with session.get(url, timeout=30) as response: if response.status == 200: html = await response.text() return parse_html(html) else: return None except Exception as e: logger.error(f"Error scraping {url}: {e}") return None ``` ### Concorrência controlada Limite a simultaneidade para evitar recursos excessivos: ```python async def scrape_with_semaphore(urls, max_concurrent=10): semaphore = asyncio.Semaphore(max_concurrent) async def _scrape_with_limit(url): async with semaphore: return await scrape_url(url) async with aiohttp.ClientSession() as session: tasks = [_scrape_with_limit(url) for url in urls] return await asyncio.gather(*tasks, return_exceptions=True) ``` ### Limitação de taxa específica de domínio Aplique limites de taxa diferentes a domínios diferentes: ```python from urllib.parse import urlparse import time import asyncio class DomainRateLimiter: def __init__(self): # Domínio -> {last_request_time, requests_per_minute} self.domains = {} self.default_rpm = 30 # Padrão: 30 solicitações por minuto def set_domain_limit(self, domínio, rpm): se o domínio não estiver em self.domains: self.domains[domínio] = {"última_solicitação_tempo": 0, "rpm": rpm} senão: self.domains[domínio]["rpm"] = rpm async def wait_if_needed(self, url): domínio = urlparse(url).netloc se o domínio não estiver em self.domains: self.domains[domínio] = {"última_solicitação_tempo": 0, "rpm": self.default_rpm} domain_info = self.domains[domínio] intervalo_mínimo = 60,0 / domain_info["rpm"] tempo_atual = time.time() elapsed = tempo_atual - domain_info["última_solicitação_tempo"] se decorrido < intervalo_mínimo: tempo_de_espera = intervalo_mínimo - decorrido await asyncio.sleep(tempo_de_espera) self.domains[domínio]["última_solicitação_tempo"] = time.time() ``` ## Otimização de análise de HTML ### Seleção do analisador Escolha o analisador mais eficiente para suas necessidades: ```python from bs4 import BeautifulSoup # lxml é muito mais rápido que html.parser html = response.text soup = BeautifulSoup(html, 'lxml') ``` ### Extração direcionada Use seletores direcionados em vez de analisar o documento inteiro: ```python # Em vez de analisar tudo soup = BeautifulSoup(html, 'lxml') links = soup.find_all('a') # Use seletores CSS para extração direcionada links = soup.select('div.content a.external') # Ou use métodos de localização mais específicos content_div = soup.find('div', class_='content') if content_div: links = content_div.find_all('a', class_='external') ``` ### Análise de streaming Para documentos grandes, use analisadores de streaming: ```python from lxml import etree def stream_parse_large_xml(file_path): """Analisar um arquivo XML grande sem carregá-lo completamente na memória.""" context = etree.iterparse(file_path, events=('end',), tag='item') for event, elem in context: # Processar o elemento process_element(elem) # Limpar o elemento para liberar memória elem.clear() # Eliminar também referências agora vazias do nó raiz para elem while elem.getprevious() is not None: del elem.getparent()[0] del context ``` ### Expressões regulares para casos simples Para extrações muito simples, regex pode ser mais rápido: ```python import re def extract_all_emails(text): """Extrair todos os e-mails do texto usando regex.""" email_pattern = r'[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\.[a-zA-Z]{2,}' return re.findall(email_pattern, text) ``` ## Gerenciamento de Recursos ### Gerenciamento de Memória Implemente estratégias para minimizar o uso de memória: ```python def process_large_dataset(file_path): """Processe um grande conjunto de dados com uso mínimo de memória.""" # Use geradores em vez de listas com open(file_path, 'r') como f: for line in f: yield process_line(line) def scrape_with_memory_limit(urls, batch_size=100): """Extraia URLs em lotes para limitar o uso de memória.""" for i in range(0, len(urls), batch_size): batch = urls[i:i+batch_size] results = scrape_batch(batch) process_and_save_results(results) # Memória livre results = None ``` ### Cache de disco Cache de respostas no disco para evitar solicitações redundantes: ```python import os import hashlib import pickle class DiskCache: def __init__(self, cache_dir='./cache'): self.cache_dir = cache_dir os.makedirs(cache_dir, exist_ok=True) def _get_cache_path(self, url): """Gere um caminho de arquivo para armazenar em cache o conteúdo da URL.""" url_hash = hashlib.md5(url.encode()).hexdigest() return os.path.join(self.cache_dir, url_hash) def get(self, url): """Recupere conteúdo do cache, se existir.""" cache_path = self._get_cache_path(url) if os.path.exists(cache_path): with open(cache_path, 'rb') como f: return pickle.load(f) return None def set(self, url, content): """Armazenar conteúdo no cache.""" cache_path = self._get_cache_path(url) com open(cache_path, 'wb') como f: pickle.dump(content, f) ``` ### Processamento incremental Processar dados incrementalmente para evitar picos de memória: ```python def incremental_scrape_and_process(urls): """Extrair e processar URLs incrementalmente.""" for url in urls: html = scrape_url(url) if html: data = extract_data(html) process_data(data) save_data(data) # Memória livre html = Nenhum data = Nenhum ``` ## Monitoramento e criação de perfil ### Métricas de desempenho Acompanhar as principais métricas de desempenho: ```python import time from dataclasses import dataclass from typing import Dict, List @dataclass class ScrapingMetrics: start_time: float = 0 end_time: float = 0 urls_processed: int = 0 successful_requests: int = 0 failed_requests: int = 0 bytes_downloaded: int = 0 domain_stats: Dict[str, Dict] = None def __post_init__(self): if self.domain_stats is None: self.domain_stats = {} def start(self): self.start_time = time.time() def stop(self): self.end_time = time.time() def add_request(self, url, success, size=0): from urllib.parse import urlparse domain = urlparse(url).netloc if domain not in self.domain_stats: self.domain_stats[domain] = { 'requests': 0, 'successful': 0, 'failed': 0, 'bytes': 0 } self.urls_processed += 1 self.domain_stats[domain]['requests'] += 1 se sucesso: self.successful_requests += 1 self.domain_stats[domain]['successful'] += 1 self.bytes_downloaded += tamanho self.domain_stats[domain]['bytes'] += tamanho senão: self.failed_requests += 1 self.domain_stats[domain]['failed'] += 1 def get_summary(self): duração = self.end_time - self.start_time se self.end_time > 0 senão tempo.tempo() - self.start_time return { 'duration_seconds': duração, 'urls_processed': self.urls_processed, 'successful_requests': self.successful_requests, 'failed_requests': self.failed_requests, 'success_rate': self.successful_requests / max(1, self.urls_processed), 'requests_per_second': self.urls_processed / max(1, duration), 'bytes_downloaded': self.bytes_downloaded, 'download_rate_kbps': (self.bytes_downloaded / 1024) / max(1, duration), 'domain_stats': self.domain_stats } ``` ### Criação de perfil Crie um perfil do seu código para identificar gargalos: ```python import cProfile import pstats import io def profile_function(func, *args, **kwargs): """Crie um perfil de uma função e exiba estatísticas.""" pr = cProfile.Profile() pr.enable() result = func(*args, **kwargs) pr.disable() s = io.StringIO() ps = pstats.Stats(pr, stream=s).sort_stats('cumulative') ps.print_stats(20) # Imprime as 20 principais funções por tempo cumulativo print(s.getvalue()) return result # Uso profile_function(scrape_batch, urls) ``` ### Registro Implementar registro detalhado para análise: ```python import logging import time # Configurar registro logging.basicConfig( level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s', filename='scraping.log' ) logger = logging.getLogger('scraper') async def scrape_with_logging(url, session): start_time = time.time() logger.info(f"Iniciando solicitação para {url}") try: async with session.get(url) como resposta: duration = time.time() - start_time size = len(await response.read()) logger.info( f"Concluído {url} - Status: {response.status}, " f"Tamanho: {size} bytes, Tempo: {duration:.2f}s" ) return await response.text() except Exception como e: duration = time.time() - start_time logger.error(f"Falha {url} - Erro: {str(e)}, Tempo: {duration:.2f}s") raise ``` ## Técnicas Especializadas ### Navegadores sem interface para sites com muito JavaScript Use navegadores sem interface para sites que exigem JavaScript: ```python from playwright.async_api import async_playwright async def scrape_js_site(url): async with async_playwright() as p: browser = await p.chromium.launch(headless=True) page = await browser.new_page() # Definir tempo limite page.set_default_timeout(30000) # Navegar e aguardar a inatividade da rede await page.goto(url, wait_until='networkidle') # Extrair conteúdo content = await page.content() # Fechar navegador await browser.close() return content ``` ### Rastreamento Inteligente Implementar estratégias de rastreamento inteligente: ```python class PriorityCrawler: def __init__(self): self.visited = set() self.queue = [] # tuplas (prioridade, url) def add_url(self, url, priority=0): if url not in self.visited: import heapq heapq.heappush(self.queue, (-priority, url)) # Negativo para heap máximo async def crawl(self, session, max_urls=100): results = {} count = 0 import heapq while self.queue and count < max_urls: _, url = heapq.heappop(self.queue) if url in self.visited: continue self.visited.add(url) count += 1 try: html = await self.fetch_url(url, session) results[url] = html # Extrai e prioriza novos links new_urls = self.extract_links(html, url) for new_url, priority in new_urls: self.add_url(new_url, priority) except Exception as e: logger.error(f"Erro ao rastrear {url}: {e}") return results def extract_links(self, html, base_url): # Extrai links e atribui prioridades com base na relevância # Retorna uma lista de tuplas (url, priority) pass ``` ### Limitação baseada em conteúdo Ajuste a velocidade de raspagem com base no tipo de conteúdo: ```python async def adaptive_scrape(url, session): """Adapte o comportamento de raspagem com base no tipo de conteúdo.""" # Primeiro, faça uma solicitação HEAD para verificar o tipo de conteúdo assíncrono com session.head(url) como head_response: content_type = head_response.headers.get('Content-Type', '') content_length = int(head_response.headers.get('Content-Length', 0)) # Ajuste o comportamento com base no conteúdo if 'text/html' in content_type: # Página HTML padrão await asyncio.sleep(1) # Atraso padrão elif 'application/json' in content_type: # Ponto de extremidade da API - pode ser mais rápido await asyncio.sleep(0.5) elif content_length > 1000000: # Arquivo grande - seja mais cauteloso await asyncio.sleep(5) else: # Comportamento padrão await asyncio.sleep(2) # Agora torne a requisição real assíncrona com session.get(url) como resposta: return await response.text() ``` ## Conclusão Otimizar as operações de scraping é um equilíbrio entre desempenho, uso de recursos e considerações éticas. Ao implementar as técnicas deste guia, você pode criar sistemas de scraping eficientes que coletam dados de forma eficaz, minimizando o impacto nos sites de destino e em seus próprios recursos. Lembre-se de que o scraper mais eficiente é aquele que: 1. Coleta apenas o que precisa; 2. Respeita os recursos do site de destino; 3. Utiliza recursos computacionais de forma eficiente; 4. Lida com erros com elegância; 5. Adapta-se a condições variáveis. Monitore sempre suas operações de scraping e esteja preparado para ajustar sua abordagem com base nas métricas de desempenho e no feedback dos sites de destino. ---Última atualização: 15/01/2025