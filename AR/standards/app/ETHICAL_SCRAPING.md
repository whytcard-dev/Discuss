# دليل الاستخلاص الأخلاقي لـ WhytCard

## مقدمة

يُعد استخلاص البيانات من الويب جوهر مشروع WhytCard، ولكن يجب إجراؤه بطريقة أخلاقية ومسؤولة وقانونية. يُحدد هذا الدليل المبادئ والممارسات الواجب اتباعها لضمان احترام جميع أنشطة الاستخلاص لحقوق مالكي المواقع الإلكترونية والقوانين المعمول بها والمعايير الأخلاقية.

## جدول المحتويات

1. [المبادئ الأساسية](#fundamental-principles)
2. [الجوانب القانونية](#legal-aspects)
3. [أفضل الممارسات التقنية](#technical-best-practices)
4. [احترام الموارد](#resource-respect)
5. [حماية البيانات الشخصية](#personal-data-protection)
6. [التوثيق والشفافية](#documentation-and-transparency)
7. [حالات خاصة](#special-cases)
8. [قائمة التحقق الأخلاقية لاستخراج البيانات](#ethical-scraping-checklist)

## المبادئ الأساسية

### فلسفة استخراج البيانات الأخلاقي

يعتمد استخراج البيانات الأخلاقي على ثلاثة مبادئ أساسية:

1. **الاحترام**: احترام مالكي المواقع الإلكترونية وشروطهم الاستخدام ومواردها

2. **التناسب**: استخراج البيانات الضرورية فقط بأقل تأثير
3. **الشفافية**: الشفافية بشأن هوية الروبوت ونوايا استخراج البيانات

### قيم WhytCard فيما يتعلق باستخراج البيانات

بصفتنا مشروع WhytCard، نلتزم بما يلي:

- عدم الإضرار بالمواقع التي نستخرج منها البيانات
- الاحترام الصارم لقواعد المواقع الإلكترونية الصريحة والضمنية
- الشفافية بشأن هويتنا وأهدافنا
- استخدام البيانات بمسؤولية ووفقًا لمهمتنا
- إعطاء الأولوية لواجهات برمجة التطبيقات الرسمية عند توفرها

## الجوانب القانونية

### الإطار القانوني العام

يخضع استخراج البيانات من الويب لعدة أطر قانونية تختلف باختلاف البلد:

- **حقوق الطبع والنشر**: محتوى الموقع الإلكتروني محمي بشكل عام بموجب حقوق الطبع والنشر
- **شروط الاستخدام**: قد تحظر شروط خدمة الموقع الإلكتروني استخراج البيانات صراحةً
- **حماية البيانات**: قوانين مثل اللائحة العامة لحماية البيانات في أوروبا تحمي البيانات الشخصية
- **غير مصرح به الوصول**: تُجرّم بعض الولايات القضائية الوصول غير المصرح به إلى أنظمة الحاسوب.

### سوابق قضائية بارزة

بعض قرارات المحاكم المهمة بشأن كشط البيانات:

- **مختبرات hiQ ضد LinkedIn** (الولايات المتحدة الأمريكية): ثَبُتَ أن كشط البيانات العامة ليس بالضرورة غير قانوني.
- **ريان إير ضد PR Aviation** (الاتحاد الأوروبي): ثَبُتَ أن شروط الاستخدام يُمكن أن تُقيّد كشط البيانات تعاقديًا.
- **QVC ضد Resultly** (الولايات المتحدة الأمريكية): شُدّد على أهمية عدم زيادة تحميل الخوادم.

### الامتثال القانوني لشركة WhytCard

للحفاظ على قانونية استخدامك:

1. **تحقق دائمًا من شروط الخدمة** قبل كشط أي موقع.
2. **احترم علامتي "noindex" و"nofollow"** في علامات التعريف.
3. **لا تتحايل أبدًا على تدابير الحماية التقنية** (CAPTCHA، قيود الوصول).
4. **وثّق ممارساتك** لإثبات حسن نيتك.
5. **استشر محاميًا** إذا كنتَ تشك في قانونية أي موقع. عملية الكشط

## أفضل الممارسات التقنية

### مراعاة ملف robots.txt

يُحدد ملف robots.txt قواعد الوصول للروبوتات:

```python
from urllib.robotparser import RobotFileParser
from urllib.parse import urlparse

def is_allowed(url, user_agent="WhytCardBot/1.0"):
"""يتحقق من إمكانية كشط عنوان URL وفقًا لملف robots.txt."""
rp = RobotFileParser()
rp.set_url(f"{urlparse(url).scheme}://{urlparse(url).netloc}/robots.txt")
rp.read()
return rp.can_fetch(user_agent, url)
```

### Proper التعريف

استخدم دائمًا وكيل مستخدم يُعرّف بوتك بوضوح:

```python
headers = {

'User-Agent': 'WhytCardBot/1.0 (+https://whytcard.com/bot; bot@whytcard.com)',
# رؤوس أخرى...
}
```

### تأخيرات الطلبات

طبّق فترات تأخير معقولة بين الطلبات:

```python
import time
import random

def polite_request(url, session, min_delay=1, max_delay=3):
"""يُقدّم طلبًا بتأخير مُهذب بين الطلبات."""
# انتظر تأخيرًا عشوائيًا
delay = random.uniform(min_delay, max_delay)
time.sleep(delay)

# أنشئ الطلب
response = session.get(url, headers=headers)
return response
```

### معالجة الأخطاء

احترم رموز أخطاء HTTP وعدّل سلوكك وفقًا لذلك:

```python
async def respectful_fetch(url, session):
"""جلب عنوان URL بطريقة محترمة."""
try:
async with session.get(url, headers=headers) as response:
if response.status == 200:
return await response.text()
elif response.status == 429: # طلبات كثيرة جدًا
# انتظر مدة أطول قبل إعادة المحاولة
wait_time = int(response.headers.get('Retry-After', 60))
logger.info(f"معدل محدود، انتظار {wait_time} ثانية")
await asyncio.sleep(wait_time)
return await respectful_fetch(url, session)
elif response.status in (403, 404):
# لا تُعِد محاولة أخطاء 403/404
logger.warning(f"تم رفض الوصول أو لم يتم العثور على: {url}")
return None
else:
# انتظر وأعِد المحاولة للأخطاء الأخرى
logger.warning(f"خطأ {response.status} لـ {url}، إعادة المحاولة خلال 5 ثوانٍ")
await asyncio.sleep(5)
return await respectful_fetch(url, session)
except Exception as e:
logger.error(f"استثناء أثناء جلب {url}: {str(e)}")
return None
```

## احترام الموارد

### تحديد المعدل

كيّف معدل طلبك مع حجم وموارد الموقع المستهدف:

- **المواقع التجارية الكبيرة**: طلب واحد كل ثانية إلى ثلاث ثوانٍ
- **المواقع متوسطة الحجم**: طلب واحد كل 3-10 ثوانٍ
- **المواقع الصغيرة**: طلب واحد كل 10-60 ثانية أو أكثر

### فترات كشط البيانات

يُفضّل فترات انخفاض حركة المرور للعمليات المكثفة:

- **ساعات خارج الذروة**: يُفضّل الليل أو عطلات نهاية الأسبوع
- **تجنب فترات الذروة**: لا تُكشط البيانات خلال فترات الذروة المعروفة
- **كن متكيفًا**: خفّض معدلك إذا لاحظت أي تباطؤ

### تقليل التأثير

تقنيات لتقليل التأثير على الخوادم المستهدفة:

1. **التخزين المؤقت الذكي**: لا تسترجع الصفحة نفسها عدة مرات
2. **الانتقائية**: استرجع الصفحات التي تحتاجها فعليًا فقط
3. **الضغط**: اطلب استجابات مضغوطة لتقليل عرض النطاق الترددي
4. **ترقيم صفحات فعال**: احترم هيكل ترقيم صفحات الموقع

## البيانات الشخصية الحماية

### تحديد البيانات الشخصية

انتبه لأنواع البيانات التي تجمعها:

- **بيانات التعريف المباشرة**: الأسماء، عناوين البريد الإلكتروني، أرقام الهواتف، العناوين
- **بيانات التعريف غير المباشرة**: معرفات المستخدمين، الأسماء المستعارة
- **البيانات الحساسة**: الآراء السياسية، الصحة، التوجه الجنسي

### مبادئ اللائحة العامة لحماية البيانات (GDPR) الواجب مراعاتها

إذا كنت تعمل في أوروبا أو تجمع بيانات من أوروبيين:

1. **التقليل**: جمع البيانات الضرورية فقط
2. **الغرض**: استخدام البيانات للأغراض المخصصة فقط
3. **الاحتفاظ المحدود**: حذف البيانات عند عدم الحاجة إليها
4. **الأمان**: حماية البيانات المجمعة من الوصول غير المصرح به

### إخفاء هوية البيانات

تقنيات إخفاء هوية البيانات الشخصية:

```python
import hashlib
import re

def anonymize_email(email):
"""يُخفي هوية البريد الإلكتروني العنوان."""
إذا لم يكن بريدًا إلكترونيًا:

إرجاع لا شيء


# تجزئة عنوان البريد الإلكتروني


hashed = hashlib.sha256(email.encode()).hexdigest()[:10]


domain = email.split('@')[-1]



return f"anon_{hashed}@{domain}"


def anonymize_phone(phone):


""يُخفي رقم الهاتف."""


إذا لم يكن هاتفًا:


إرجاع لا شيء


# الاحتفاظ بالأرقام فقط


digits = re.sub(r'\D', '', phone)



# إخفاء جميع الأرقام باستثناء آخر رقمين


if len(digits) > 2:


return "X" * (len(digits) - 2) + digits[-2:]


return "X" * len(digits)


```


## التوثيق و الشفافية

### توثيق أنشطة الكشط

وثّق دائمًا أنشطة الكشط:

- **الغرض**: لماذا تُجمع هذه البيانات؟
- **الطريقة**: كيف تُجمع؟
- **التخزين**: أين وكيف تُخزّن؟
- **الاستخدام**: كيف ستُستخدم؟
- **الحذف**: متى ستُحذف؟

### الاتصال وإلغاء الاشتراك

احرص دائمًا على توفير طريقة للتواصل معك:

1. **صفحة المعلومات**: أنشئ صفحة مخصصة لشرح برنامج البوت الخاص بك (مثل whytcard.com/bot)
2. **بريد التواصل الإلكتروني**: أدخل عنوان بريدك الإلكتروني في وكيل المستخدم الخاص بك
3. **آلية إلغاء الاشتراك**: اسمح للمواقع بطلب الاستبعاد

### تسجيل النشاط

احتفظ بسجلات مفصلة لأنشطة استخراج البيانات:

```python
import logging
from datetime import datetime

# تكوين المسجل
logging.basicConfig(
filename=f"scraping_log_{datetime.now().strftime('%Y%m%d')}.log",
level=logging.INFO,
format='%(asctime)s - %(levelname)s - %(message)s'
)

def log_scraping_activity(url, success, data_points=0):
"""يُسجِّل نشاط كشط البيانات."""
logging.info(f"URL: {url}, Success: {success}, Data points: {data_points}")
```

## حالات خاصة

### واجهة برمجة التطبيقات (API) مقابل كشط البيانات (Scraping)

ترتيب أولوية جمع البيانات:

1. **واجهات برمجة التطبيقات الرسمية**: أعطِ الأولوية دائمًا لواجهات برمجة التطبيقات الرسمية عند وجودها.
2. **مصادر البيانات العامة**: استخدم مصادر RSS أو XML أو JSON إن وجدت.
3. **كشط البيانات**: استخدم كشط البيانات فقط كملاذ أخير.

### المواقع التي تتطلب مصادقة

للمواقع التي تتطلب مصادقة:

- **الموافقة الصريحة**: احصل على موافقة كتابية من الموقع.
- **احترام شروط الخدمة**: تأكد من أن شروط الخدمة تسمح بالاستخدام الآلي.
- **القيود**: احترم الاستخدام تمامًا. القيود

### المحتوى الديناميكي (جافا سكريبت)

للمواقع التي تستخدم جافا سكريبت بكثرة:

```python
from playwright.async_api import async_playwright

async def scrape_dynamic_content(url):
"""استخراج المحتوى المُولّد بواسطة جافا سكريبت."""
async with async_playwright() as p:
browse = await p.chromium.launch(headless=True)
page = await browse.new_page()

# تهيئة وكيل المستخدم
await page.set_extra_http_headers({ 
'User-Agent': 'WhytCardBot/1.0 (+https://whytcard.com/bot)'
})

# تحميل الصفحة والانتظار حتى تصبح الشبكة خاملة
await page.goto(url)
await page.wait_for_load_state('networkidle')

# استخراج المحتوى
content = await page.content()

await Browser.close()
return content
```

## قائمة التحقق الأخلاقية لاستخراج البيانات

قبل كل مشروع استخراج بيانات، تحقق من النقاط التالية:

### التحضير
- [ ] تحقق من شروط الخدمة للموقع المستهدف
- [ ] تحقق من ملف robots.txt
- [ ] ابحث عن واجهة برمجة تطبيقات أو بدائل لاستخراج البيانات
- [ ] تعريف واضح للبيانات اللازمة
- [ ] توثيق غرض استخراج البيانات

### التكوين الفني
- [ ] وكيل مستخدم قابل للتعريف وشفاف
- [ ] آلية تحديد السرعة
- [ ] نظام تخزين مؤقت لتجنب الطلبات المكررة
- [ ] معالجة مناسبة للأخطاء وأكواد HTTP
- [ ] تسجيل النشاط

### التنفيذ
- [ ] مراقبة أداء الموقع المستهدف
- [ ] تعديل المعدل الديناميكي عند الحاجة
- [ ] مراعاة مؤشرات الخادم (429، إعادة المحاولة بعد)
- [ ] التوقف الفوري في حال اكتشاف مشكلة

### المعالجة اللاحقة
- [ ] إخفاء هوية البيانات الشخصية
- [ ] تخزين آمن للبيانات
- [ ] الاحتفاظ بها لفترة محدودة
- [ ] توثيق البيانات المجمعة

## الخاتمة

يُمثل استخراج البيانات الأخلاقي توازنًا بين الوصول إلى البيانات واحترام حقوق وموارد مالكي المواقع الإلكترونية. باتباع هذه المبادئ والممارسات، يُمكن لمشروع WhytCard جمع البيانات اللازمة مع الحفاظ على نهج مسؤول ومحترم.

تذكر أن أخلاقيات استخراج البيانات ليست مجرد مسألة امتثال قانوني، بل هي أيضًا مسؤولية تجاه منظومة الويب ككل. يُسهم استخراج البيانات باحترام في توفير شبكة ويب أكثر انفتاحًا واستدامة للجميع.

---

آخر تحديث: 2025-01-15 