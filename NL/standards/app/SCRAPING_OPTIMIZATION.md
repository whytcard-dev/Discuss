# Handleiding voor scrapingoptimalisatie ## Inleiding Webscraping is een fundamenteel onderdeel van het WhytCard-project, maar het kan veel resources vergen en prestatieproblemen opleveren. Deze handleiding beschrijft strategieën en best practices voor het optimaliseren van scrapingbewerkingen om de efficiëntie te maximaliseren en tegelijkertijd het resourcegebruik en de impact op de doelwebsites te minimaliseren. ## Inhoudsopgave 1. [Fundamentele principes](#fundamental-principles) 2. [Gedistribueerde architectuur](#distributed-architecture) 3. [HTTP-aanvraagoptimalisatie](#http-request-optimization) 4. [Parallelisatie en gelijktijdigheid](#parallelisatie-en-gelijktijdigheid) 5. [HTML-parsingoptimalisatie](#html-parsing-optimization) 6. [Resourcebeheer](#resource-management) 7. [Monitoring en profilering](#monitoring-en-profilering) 8. [Gespecialiseerde technieken](#specialized-techniques) ## Fundamentele principes ### Efficiëntie versus beleefdheid Scrapingoptimalisatie moet een evenwicht vinden tussen twee soms tegenstrijdige doelen: 1. **Efficiëntie**: De snelheid van gegevensverzameling en het gebruik van bronnen maximaliseren 2. **Beleefdheid**: De impact op doelwebsites minimaliseren en met respect voor hun middelen Geef altijd voorrang aan het goede webburgerschap boven pure prestaties wanneer deze doelen met elkaar in conflict zijn. ### Belangrijkste statistieken Concentreer u bij het optimaliseren van scrapbewerkingen op deze belangrijke statistieken: - **Pagina's per minuut**: Snelheid van paginaverzameling - **CPU-gebruik**: Verwerkingsoverhead - **Geheugengebruik**: RAM-verbruik - **Netwerkefficiëntie**: Bandbreedtegebruik - **Foutpercentage**: Percentage mislukte verzoeken - **Impact op doelserver**: Belasting van gescrapte sites ## Gedistribueerde architectuur ### Taakverdeling Verdeel taken over meerdere workers voor scraping op grote schaal: ```python # Voorbeeld waarbij Celery wordt gebruikt voor gedistribueerd scrapen vanuit Celery import Celery app = Celery('scraping_tasks', broker='redis://localhost:6379/0') @app.task def scrape_url(url): # Scrapinglogica retourneert resultaat # Taken verzenden urls = ["https://example1.com", "https://example2.com", "https://example3.com"] results = [scrape_url.delay(url) voor url in url's] ``` ### Load Balancing Implementeer load balancing om verzoeken te verdelen over meerdere IP-adressen of instanties: ```python-klasse LoadBalancer: def __init__(self, proxies): self.proxies = proxies self.current_index = 0 def get_next_proxy(self): proxy = self.proxies[self.current_index] self.current_index = (self.current_index + 1) % len(self.proxies) return proxy ``` ### Proxy-rotatie Gebruik proxy-rotatie om op IP gebaseerde snelheidsbeperking te vermijden: ```python async def fetch_with_proxy_rotation(url, proxy_manager, session): max_retries = 3 retry_count = 0 while retry_count < max_retries: proxy = proxy_manager.get_next_proxy() try: async with session.get(url, proxy=proxy, timeout=30) als respons: if response.status == 200: return wait response.text() elif response.status == 429: # Te veel verzoeken # Markeer deze proxy als rate-limited proxy_manager.mark_rate_limited(proxy) retry_count += 1 else: retry_count += 1 except Exception as e: # Markeer deze proxy als mislukt proxy_manager.mark_failed(proxy) retry_count += 1 raise Exception(f"Kan {url} niet ophalen na {max_retries} pogingen") ``` ## Optimalisatie van HTTP-verzoeken ### Verbindingspooling Hergebruik HTTP-verbindingen om de overhead te verminderen: ```python async def scrape_with_connection_pooling(): # Maak één sessie voor meerdere verzoeken asynchroon met aiohttp.ClientSession() als sessie: tasks = [] for url in urls: tasks.append(fetch(url, session)) return wait asyncio.gather(*tasks) async def fetch(url, session): async met session.get(url) als respons: return wait response.text() ``` ### HTTP/2-ondersteuning Gebruik HTTP/2 indien beschikbaar om te profiteren van multiplexing: ```python import httpx async def fetch_with_http2(): async met httpx.AsyncClient(http2=True) als client: response = wait client.get("https://example.com") return response.text ``` ### Compressie Vraag gecomprimeerde responsen aan om het bandbreedtegebruik te verminderen: ```python headers = { 'Accept-Encoding': 'gzip, deflate, br', 'User-Agent': 'WhytCardBot/1.0' } async def fetch_with_compression(url, session): async met session.get(url, headers=headers) als antwoord: return wait response.text() ``` ### Optimalisatie van aanvragen Vraag alleen aan wat u nodig hebt: ```python # Vraag alleen de benodigde headers aan headers = { 'Accept': 'text/html', 'Accept-Language': 'en-US,en;q=0.5', 'Accept-Encoding': 'gzip, deflate, br', 'User-Agent': 'WhytCardBot/1.0' } # Gebruik HEAD-aanvragen om bronnen te controleren voordat u een async GET uitvoert def check_before_download(url, session): async with session.head(url) als antwoord: if response.status == 200 and response.headers.get('Content-Type') == 'text/html': return wait fetch_full_page(url, session) return None ``` ## Parallellisatie en gelijktijdigheid ### Asynchroon scrapen Gebruik asynchrone programmering om meerdere aanvragen te verwerken gelijktijdig: ```python import async import aiohttp async def scrape_all(urls): async met aiohttp.ClientSession() als sessie: taken = [scrape_one(url, sessie) voor url in urls] return wait asyncio.gather(*taken, return_exceptions=True) async def scrape_one(url, sessie): probeer: async met session.get(url, timeout=30) als antwoord: als response.status == 200: html = wacht op response.text() return parse_html(html) anders: return None behalve Exception als e: logger.error(f"Fout bij het scrapen van {url}: {e}") return None ``` ### Gecontroleerde gelijktijdigheid Beperk gelijktijdigheid om overbelasting van bronnen te voorkomen: ```python async def scrape_with_semaphore(urls, max_concurrent=10): semaphore = async.Semaphore(max_concurrent) async def _scrape_with_limit(url): async met semaphore: return wait scrape_url(url) async met aiohttp.ClientSession() as session: tasks = [_scrape_with_limit(url) voor url in urls] return wait asyncio.gather(*tasks, return_exceptions=True) ``` ### Domeinspecifieke snelheidsbeperking Pas verschillende snelheidslimieten toe op verschillende domeinen: ```python from urllib.parse import urlparse import time import asyncio class DomainRateLimiter: def __init__(self): # Domein -> {laatste_aanvraag_tijd, verzoeken_per_minuut} self.domains = {} self.default_rpm = 30 # Standaard: 30 verzoeken per minuut def set_domain_limit(self, domein, rpm): als domein niet in self.domains: self.domains[domein] = {"laatste_aanvraagtijd": 0, "rpm": rpm} anders: self.domains[domein]["rpm"] = rpm async def wait_if_needed(self, url): domein = urlparse(url).netloc als domein niet in self.domains: self.domains[domein] = {"laatste_aanvraagtijd": 0, "rpm": self.default_rpm} domain_info = self.domains[domein] min_interval = 60.0 / domain_info["rpm"] huidige_tijd = time.time() verstreken = huidige_tijd - domain_info["laatste_aanvraagtijd"] als verstreken < min_interval: wait_time = min_interval - verstreken wait asyncio.sleep(wait_time) self.domains[domain]["last_request_time"] = time.time() ``` ## Optimalisatie van HTML-parsering ### Parserselectie Kies de meest efficiënte parser voor uw behoeften: ```python van bs4 import BeautifulSoup # lxml is veel sneller dan html.parser html = response.text soup = BeautifulSoup(html, 'lxml') ``` ### Gerichte extractie Gebruik gerichte selectoren in plaats van het hele document te parsen: ```python # In plaats van alles te parsen soup = BeautifulSoup(html, 'lxml') links = soup.find_all('a') # Gebruik CSS-selectoren voor gerichte extractie links = soup.select('div.content a.external') # Of gebruik specifiekere zoekmethoden content_div = soup.find('div', class_='content') if content_div: links = content_div.find_all('a', class_='external') ``` ### Streaming-parsing Voor grote documenten gebruikt u streaming-parsers: ```python from lxml import etree def stream_parse_large_xml(file_path): """Een groot XML-bestand parseren zonder het volledig in het geheugen te laden.""" context = etree.iterparse(file_path, events=('end',), tag='item') for event, elem in context: # Verwerk het element process_element(elem) # Wis element om geheugen vrij te maken elem.clear() # Verwijder ook nu lege verwijzingen van het hoofdknooppunt naar elem terwijl elem.getprevious() niet None is: del elem.getparent()[0] del context ``` ### Reguliere expressies voor eenvoudige gevallen Voor zeer eenvoudige extracties kan regex sneller zijn: ```python import re def extract_all_emails(text): """Alle e-mails uit tekst extraheren met behulp van regex.""" email_pattern = r'[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\.[a-zA-Z]{2,}' return re.findall(email_pattern, text) ``` ## Resourcebeheer ### Geheugenbeheer Implementeer strategieën om het geheugengebruik te minimaliseren: ```python def process_large_dataset(file_path): """Een grote dataset verwerken met minimaal geheugengebruik.""" # Generators gebruiken in plaats van lijsten met open(file_path, 'r') as f: for line in f: yield process_line(line) def scrape_with_memory_limit(urls, batch_size=100): """URL's in batches schrapen om het geheugengebruik te beperken.""" for i in range(0, len(urls), batch_size): batch = urls[i:i+batch_size] results = scrape_batch(batch) process_and_save_results(results) # Geheugen vrijmaken results = None ``` ### Schijfcaching Cachereacties op schijf om redundante verzoeken te voorkomen: ```python import os import hashlib import pickle class DiskCache: def __init__(self, cache_dir='./cache'): self.cache_dir = cache_dir os.makedirs(cache_dir, exist_ok=True) def _get_cache_path(self, url): """Genereer een bestandspad voor het cachen van de URL-inhoud.""" url_hash = hashlib.md5(url.encode()).hexdigest() return os.path.join(self.cache_dir, url_hash) def get(self, url): """Haal inhoud op uit de cache als deze bestaat.""" cache_path = self._get_cache_path(url) als os.path.exists(cache_path): met open(cache_path, 'rb') als f: return pickle.load(f) return None def set(self, url, content): """Inhoud in cache opslaan.""" cache_path = self._get_cache_path(url) met open(cache_path, 'wb') als f: pickle.dump(content, f) ``` ### Incrementele verwerking Gegevens incrementeel verwerken om geheugenpieken te voorkomen: ```python def incremental_scrape_and_process(urls): """URL's incrementeel schrapen en verwerken.""" for url in urls: html = scrape_url(url) als html: data = extract_data(html) process_data(data) save_data(data) # Geheugen vrijmaken html = None data = None ``` ## Monitoring en profilering ### Prestatiegegevens Houd de belangrijkste prestatiegegevens bij: ```python importeer tijd van dataclasses importeer dataclass van typen importeer Dict, lijst @dataclass klasse ScrapingMetrics: start_tijd: float = 0 eind_tijd: float = 0 urls_processed: int = 0 succesvolle_aanvragen: int = 0 mislukte_aanvragen: int = 0 bytes_downloaded: int = 0 domein_statistieken: Dict[str, Dict] = None def __post_init__(self): als self.domein_stats None is: self.domein_stats = {} def start(self): self.start_tijd = time.time() def stop(self): self.end_tijd = time.time() def add_request(self, url, succes, grootte=0): van urllib.parse importeer urlparse domein = urlparse(url).netloc als domein niet in self.domain_stats: self.domain_stats[domein] = { 'aanvragen': 0, 'geslaagd': 0, 'mislukt': 0, 'bytes': 0 } self.urls_processed += 1 self.domain_stats[domein]['aanvragen'] += 1 indien geslaagd: self.succesvolle_aanvragen += 1 self.domain_stats[domein]['succesvol'] += 1 self.bytes_downloaded += grootte self.domain_stats[domein]['bytes'] += grootte anders: self.mislukte_aanvragen += 1 self.domain_stats[domein]['mislukt'] += 1 def get_summary(self): duur = self.end_time - self.start_time indien self.end_time > 0 anders time.time() - self.start_time return { 'duration_seconds': duur, 'urls_processed': self.urls_processed, 'succesvolle_aanvragen': self.succesvolle_aanvragen, 'mislukte_aanvragen': self.mislukte_aanvragen, 'succespercentage': self.succesvolle_aanvragen / max(1, self.urls_processed), 'aanvragen_per_seconde': self.urls_processed / max(1, duur), 'bytes_downloaded': self.bytes_downloaded, 'download_rate_kbps': (self.bytes_downloaded / 1024) / max(1, duur), 'domein_statistieken': self.domein_statistieken } ``` ### Profilering Profileer uw code om knelpunten te identificeren: ```python import cProfile import pstats import io def profile_function(func, *args, **kwargs): """Profileer een functie en druk statistieken af.""" pr = cProfile.Profile() pr.enable() result = func(*args, **kwargs) pr.disable() s = io.StringIO() ps = pstats.Stats(pr, stream=s).sort_stats('cumulative') ps.print_stats(20) # Druk de top 20 functies af op cumulatieve tijd print(s.getvalue()) return result # Gebruik profile_function(scrape_batch, urls) ``` ### Logging Implementeer gedetailleerde logging voor analyse: ```python import logging import time # Configureer logging logging.basicConfig( level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s', filename='scraping.log' ) logger = logging.getLogger('scraper') async def scrape_with_logging(url, session): start_time = time.time() logger.info(f"Start verzoek naar {url}") try: async met session.get(url) as response: duration = time.time() - start_time size = len(await response.read()) logger.info( f"Voltooid {url} - Status: {response.status}, " f"Grootte: {size} bytes, Tijd: {duration:.2f}s" ) return wait response.text() except Exception as e: duration = time.time() - start_time logger.error(f"Mislukt {url} - Fout: {str(e)}, Tijd: {duration:.2f}s") raise ``` ## Gespecialiseerde technieken ### Headless browsers voor sites met veel JavaScript Gebruik headless browsers voor sites die JavaScript vereisen: ```python van playwright.async_api import async_playwright async def scrape_js_site(url): async met async_playwright() als p: browser = wacht op p.chromium.launch(headless=True) pagina = wacht op browser.new_page() # Stel time-out in page.set_default_timeout(30000) # Navigeer en wacht op netwerk-inactiviteit wacht op page.goto(url, wait_until='networkidle') # Inhoud extraheren content = wacht op page.content() # Browser sluiten wacht op browser.close() retourneer inhoud ``` ### Intelligent crawlen Intelligente crawlstrategieën implementeren: ```python-klasse PriorityCrawler: def __init__(self): self.visited = set() self.queue = [] # (prioriteit, url) tupels def add_url(self, url, prioriteit=0): als url niet in self.visited: import heapq heapq.heappush(self.queue, (-priority, url)) # Negatief voor max-heap async def crawl(self, session, max_urls=100): results = {} count = 0 import heapq while self.queue and count < max_urls: _, url = heapq.heappop(self.queue) if url in self.visited: continue self.visited.add(url) count += 1 try: html = wait self.fetch_url(url, session) results[url] = html # Nieuwe links extraheren en prioriteren new_urls = self.extract_links(html, url) for new_url, priority in new_urls: self.add_url(new_url, priority) except Exception as e: logger.error(f"Fout bij het crawlen van {url}: {e}") return results def extract_links(self, html, base_url): # Extraheer links en wijs prioriteiten toe op basis van relevantie # Retourneer lijst met (url, prioriteit) tupels pass ``` ### Content-based Throttling Pas de scrapingsnelheid aan op basis van het type inhoud: ```python async def adaptive_scrape(url, session): """Pas het scrapinggedrag aan op basis van het type inhoud.""" # Maak eerst een HEAD-aanvraag om te controleren of het type inhoud asynchroon is met session.head(url) als head_response: content_type = head_response.headers.get('Content-Type', '') content_length = int(head_response.headers.get('Content-Length', 0)) # Pas het gedrag aan op basis van inhoud als 'text/html' in content_type: # Standaard HTML-pagina wait asyncio.sleep(1) # Standaardvertraging elif 'application/json' in content_type: # API-eindpunt - kan sneller zijn wait asyncio.sleep(0.5) elif content_length > 1000000: # Groot bestand - wees voorzichtiger wait asyncio.sleep(5) else: # Standaardgedrag wait asyncio.sleep(2) # Maak nu de daadwerkelijke aanvraag asynchroon met session.get(url) als respons: return wait response.text() ``` ## Conclusie Het optimaliseren van scrapingbewerkingen is een evenwicht tussen prestaties, resourcegebruik en ethische overwegingen. Door de technieken in deze handleiding te implementeren, kunt u efficiënte scrapingsystemen creëren die effectief gegevens verzamelen en tegelijkertijd de impact op de doelwebsites en uw eigen resources minimaliseren. Onthoud dat de meest efficiënte scraper er een is die: 1. Alleen verzamelt wat nodig is 2. De resources van de doelwebsite respecteert 3. Computationele resources efficiënt gebruikt 4. Fouten netjes verwerkt 5. Zich aanpast aan veranderende omstandigheden Houd uw scrapingbewerkingen altijd in de gaten en wees voorbereid om uw aanpak aan te passen op basis van prestatiemetingen en feedback van de doelwebsites. ---Laatst bijgewerkt: 2025-01-15