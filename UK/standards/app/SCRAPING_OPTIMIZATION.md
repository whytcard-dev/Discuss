# Посібник з оптимізації парсингу ## Вступ Веб-парсинг є фундаментальним компонентом проекту WhytCard, але він може бути ресурсомістким і створювати проблеми з продуктивністю. У цьому посібнику описано стратегії та найкращі практики оптимізації операцій парсингу для максимізації ефективності, мінімізації використання ресурсів та впливу на цільові веб-сайти. ## Зміст 1. [Основні принципи](#fundamental-principles) 2. [Розподілена архітектура](#distributed-architecture) 3. [Оптимізація HTTP-запитів](#http-request-optimization) 4. [Паралелізація та паралелізм](#parallelization-and-concurrency) 5. [Оптимізація парсингу HTML](#html-parsing-optimization) 6. [Керування ресурсами](#resource-management) 7. [Моніторинг та профілювання](#monitoring-and-profiling) 8. [Спеціалізовані методи](#specialized-techniques) ## Основні принципи ### Ефективність проти ввічливості Оптимізація парсингу повинна збалансувати дві іноді суперечливі цілі: 1. **Ефективність**: Максимізація швидкості збору даних та використання ресурсів 2. **Ввічливість**: Мінімізація впливу на цільові веб-сайти та повага до їхніх ресурсів Завжди надавайте пріоритет хорошому веб-громадянину, а не чистій продуктивності, коли ці цілі суперечать одна одній. ### Ключові показники Під час оптимізації операцій парсингу зосередьтеся на цих ключових показниках: - **Сторінок за хвилину**: Швидкість збору сторінок - **Використання процесора**: Накладні витрати на обробку - **Використання пам'яті**: Споживання оперативної пам'яті - **Ефективність мережі**: Використання пропускної здатності - **Коефіцієнт помилок**: Відсоток невдалих запитів - **Вплив цільового сервера**: Навантаження, що розміщується на парсованих сайтах ## Розподілена архітектура ### Розподіл завдань Для великомасштабного парсингу розподіліть завдання між кількома виконавцями: ```python # Приклад використання Celery для розподіленого парсингу з celery import Celery app = Celery('scraping_tasks', broker='redis://localhost:6379/0') @app.task def scrape_url(url): # Логіка парсингу return result # Відправлення завдань urls = ["https://example1.com", "https://example2.com", "https://example3.com"] results = [scrape_url.delay(url) for url in urls] ``` ### Балансування навантаження Реалізуйте балансування навантаження для розподілу запитів між кількома IP-адресами або екземпляри: ```python class LoadBalancer: def __init__(self, proxies): self.proxies = proxies self.current_index = 0 def get_next_proxy(self): proxy = self.proxies[self.current_index] self.current_index = (self.current_index + 1) % len(self.proxies) return proxy ``` ### Ротація проксі-сервера Використовуйте ротацію проксі-сервера, щоб уникнути обмеження швидкості на основі IP: ```python async def fetch_with_proxy_rotation(url, proxy_manager, session): max_retries = 3 retry_count = 0 while retry_count < max_retries: proxy = proxy_manager.get_next_proxy() try: async with session.get(url, proxy=proxy, timeout=30) as response: if response.status == 200: return await response.text() elif response.status == 429: # Забагато запитів # Позначити цей проксі як такий, що має обмеження швидкості proxy_manager.mark_rate_limited(proxy) retry_count += 1 else: retry_count += 1 except Exception as e: # Позначити цей проксі як такий, що не вдався proxy_manager.mark_failed(proxy) retry_count += 1 raise Exception(f"Не вдалося отримати {url} після {max_retries} спроб") ``` ## Оптимізація HTTP-запитів ### Пулінг підключень Повторно використовуйте HTTP-з'єднання для зменшення накладних витрат: ```python async def scrape_with_connection_pooling(): # Створіть один сеанс для кількох запитів async with aiohttp.ClientSession() as session: tasks = [] for url in urls: tasks.append(fetch(url, session)) return await asyncio.gather(*tasks) async def fetch(url, session): async with session.get(url) as response: return await response.text() ``` ### Підтримка HTTP/2 Використовуйте HTTP/2, коли це можливо, щоб скористатися перевагами мультиплексування: ```python import httpx async def fetch_with_http2(): async with httpx.AsyncClient(http2=True) as client: response = await client.get("https://example.com") return response.text ``` ### Стиснення Запит стиснутих відповідей для зменшення використання пропускної здатності: ```python headers = { 'Accept-Encoding': 'gzip, deflate, br', 'User-Agent': 'WhytCardBot/1.0' } async def fetch_with_compression(url, session): async with session.get(url, headers=headers) as response: return await response.text() ``` ### Оптимізація запитів Запитуйте лише те, що вам потрібно: ```python # Запитуйте лише необхідні заголовки headers = { 'Accept': 'text/html', 'Accept-Language': 'en-US,en;q=0.5', 'Accept-Encoding': 'gzip, deflate, br', 'User-Agent': 'WhytCardBot/1.0' } # Використовуйте HEAD-запити для перевірки ресурсів перед GET async def check_before_download(url, session): async with session.head(url) as response: if response.status == 200 and response.headers.get('Content-Type') == 'text/html': return await fetch_full_page(url, session) return None ``` ## Паралелізація та паралелізм ### Асинхронний скрейпінг Використовуйте асинхронне програмування для одночасної обробки кількох запитів: ```python import asyncio import aiohttp async def scrape_all(urls): async with aiohttp.ClientSession() as session: tasks = [scrape_one(url, session) for url in urls] return await asyncio.gather(*tasks, return_exceptions=True) async def scrape_one(url, session): try: async with session.get(url, timeout=30) as response: if response.status == 200: html = await response.text() return parse_html(html) else: return None except Exception as e: logger.error(f"Помилка скрапінгу {url}: {e}") return None ``` ### Контрольований паралелізм Обмеження паралельності, щоб уникнути перевантаження ресурсів: ```python async def scrape_with_semaphore(urls, max_concurrent=10): semaphore = asyncio.Semaphore(max_concurrent) async def _scrape_with_limit(url): async with semaphore: return await scrape_url(url) async with aiohttp.ClientSession() as session: tasks = [_scrape_with_limit(url) for url in urls] return await asyncio.gather(*tasks, return_exceptions=True) ``` ### Обмеження швидкості для домену Застосування різних обмежень швидкості до різних домени: ```python from urllib.parse import urlparse import time import asyncio class DomainRateLimiter: def __init__(self): # Домен -> {last_request_time, requests_per_minute} self.domains = {} self.default_rpm = 30 # За замовчуванням: 30 запитів на хвилину def set_domain_limit(self, domain, rpm): якщо домену немає в self.domains: self.domains[domain] = {"last_request_time": 0, "rpm": rpm} else: self.domains[domain]["rpm"] = rpm async def wait_if_needed(self, url): domain = urlparse(url).netloc якщо домену немає в self.domains: self.domains[domain] = {"last_request_time": 0, "rpm": self.default_rpm} domain_info = self.domains[domain] min_interval = 60.0 / domain_info["rpm"] current_time = time.time() elapsed = current_time - domain_info["last_request_time"] if elapsed < min_interval: wait_time = min_interval - elapsed await asyncio.sleep(wait_time) self.domains[domain]["last_request_time"] = time.time() ``` ## Оптимізація парсингу HTML ### Вибір парсера Виберіть найефективніший парсер для ваших потреб: ```python from bs4 import BeautifulSoup # lxml набагато швидший за html.parser html = response.text soup = BeautifulSoup(html, 'lxml') ``` ### Цільове вилучення Використовуйте цільові селектори замість парсингу всього документа: ```python # Замість парсингу всього soup = BeautifulSoup(html, 'lxml') links = soup.find_all('a') # Використовуйте CSS селектори для цільового вилучення links = soup.select('div.content a.external') # Або використовуйте більш специфічні методи пошуку content_div = soup.find('div', class_='content') if content_div: links = content_div.find_all('a', class_='external') ``` ### Потоковий парсинг Для великих документів використовуйте потокові парсери: ```python from lxml import etree def stream_parse_large_xml(file_path): """Розбирати великий XML-файл без повного завантаження його в пам'ять.""" context = etree.iterparse(file_path, events=('end',), tag='item') for event, elem in context: # Обробляти елемент process_element(elem) # Очищати елемент для звільнення пам'яті elem.clear() # Також видаляти порожні посилання з кореневого вузла на elem, поки elem.getprevious() не є None: del elem.getparent()[0] del context ``` ### Регулярні вирази для простих випадків Для дуже простих вилучень регулярний вираз може бути швидшим: ```python import re def extract_all_emails(text): """Витягнути всі електронні листи з тексту за допомогою регулярного виразу.""" email_pattern = r'[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\.[a-zA-Z]{2,}' return re.findall(email_pattern, text) ``` ## Керування ресурсами ### Керування пам'яттю Впроваджуйте стратегії для мінімізації використання пам'яті: ```python def process_large_dataset(file_path): """Обробляйте великий набір даних з мінімальним використанням пам'яті.""" # Використовуйте генератори замість списків with open(file_path, 'r') as f: for line in f: yield process_line(line) def scrape_with_memory_limit(urls, batch_size=100): """Пакетно витягуйте URL-адреси, щоб обмежити використання пам'яті.""" for i in range(0, len(urls), batch_size): batch = urls[i:i+batch_size] results = scrape_batch(batch) process_and_save_results(results) # Вільна пам'ять results = None ``` ### Кешування диска Кешує відповіді на диск, щоб уникнути надлишкових запитів: ```python import os import hashlib import pickle class DiskCache: def __init__(self, cache_dir='./cache'): self.cache_dir = cache_dir os.makedirs(cache_dir, exist_ok=True) def _get_cache_path(self, url): """Згенерувати шлях до файлу для кешування вмісту URL.""" url_hash = hashlib.md5(url.encode()).hexdigest() return os.path.join(self.cache_dir, url_hash) def get(self, url): """Отримати вміст з кешу, якщо він існує.""" cache_path = self._get_cache_path(url) if os.path.exists(cache_path): with open(cache_path, 'rb') as f: return pickle.load(f) return None def set(self, url, content): """Зберігати вміст у кеші.""" cache_path = self._get_cache_path(url) with open(cache_path, 'wb') as f: pickle.dump(content, f) ``` ### Інкрементальна обробка Обробляти дані інкрементально, щоб уникнути піків пам'яті: ```python def incremental_scrape_and_process(urls): """Скрейпувати та обробляти URL-адреси інкрементально.""" for url in urls: html = scrape_url(url) if html: data = extract_data(html) process_data(data) save_data(data) # Звільнити пам'ять html = None data = None ``` ## Моніторинг та профілювання ### Метрики продуктивності Відстежувати ключові показники продуктивності: ```python import time from dataclasses import dataclass from typing import Dict, List @dataclass class ScrapingMetrics: start_time: float = 0 end_time: float = 0 urls_processed: int = 0 successful_requests: int = 0 failed_requests: int = 0 bytes_downloaded: int = 0 domain_stats: Dict[str, Dict] = None def __post_init__(self): if self.domain_stats is None: self.domain_stats = {} def start(self): self.start_time = time.time() def stop(self): self.end_time = time.time() def add_request(self, url, success, size=0): from urllib.parse import urlparse domain = urlparse(url).netloc if domain not in self.domain_stats: self.domain_stats[domain] = { 'requests': 0, 'успішно': 0, 'не вдалося': 0, 'байти': 0 } self.urls_processed += 1 self.domain_stats[домен]['запитів'] += 1 якщо успіх: self.successful_requests += 1 self.domain_stats[домен]['успішно'] += 1 self.bytes_downloaded += розмір self.domain_stats[домен]['байтів'] += розмір else: self.failed_requests += 1 self.domain_stats[домен]['не вдалося'] += 1 def get_summary(self): duration = self.end_time - self.start_time якщо self.end_time > 0 else time.time() - self.start_time return { 'duration_seconds': duration, 'urls_processed': self.urls_processed, 'successful_requests': self.successful_requests, 'failed_requests': self.failed_requests, 'success_rate': self.successful_requests / max(1, self.urls_processed), 'requests_per_second': self.urls_processed / max(1, duration), 'bytes_downloaded': self.bytes_downloaded, 'download_rate_kbps': (self.bytes_downloaded / 1024) / max(1, duration), 'domain_stats': self.domain_stats } ``` ### Профілювання Профілювання вашого коду для виявлення вузьких місць: ```python import cProfile import pstats import io def profile_function(func, *args, **kwargs): """Профілювання функції та виведення статистики.""" pr = cProfile.Profile() pr.enable() result = func(*args, **kwargs) pr.disable() s = io.StringIO() ps = pstats.Stats(pr, stream=s).sort_stats('cumulative') ps.print_stats(20) # Вивести топ-20 функцій за кумулятивним часом print(s.getvalue()) return result # Використання profile_function(scrape_batch, urls) ``` ### Журнал Реалізація детального журналювання для аналізу: ```python import logging import time # Налаштування журналювання logging.basicConfig( level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s', filename='scraping.log' ) logger = logging.getLogger('scraper') async def scrape_with_logging(url, session): start_time = time.time() logger.info(f"Початок запиту до {url}") try: async with session.get(url) as response: duration = time.time() - start_time size = len(await response.read()) logger.info( f"Завершено {url} - Статус: {response.status}, " f"Розмір: {size} байтів, Час: {duration:.2f}s" ) return await response.text() except Exception as e: duration = time.time() - start_time logger.error(f"Не вдалося {url} - Помилка: {str(e)}, Час: {duration:.2f}s") raise ``` ## Спеціалізовані методи ### Безголові браузери для сайтів з великою кількістю JavaScript Використовуйте безголові браузери для сайтів, що потребують JavaScript: ```python from drammar.async_api import async_playwright async def scrape_js_site(url): async with async_playwright() as p: browser = await p.chromium.launch(headless=True) page = await browser.new_page() # Встановити тайм-аут page.set_default_timeout(30000) # Навігація та очікування простою мережі await page.goto(url, wait_until='networkidle') # Вилучення контенту content = await page.content() # Закриття браузера await browser.close() return content ``` ### Інтелектуальне сканування Реалізація інтелектуальних стратегій сканування: ```python class PriorityCrawler: def __init__(self): self.visited = set() self.queue = [] # (priority, url) кортежі def add_url(self, url, priority=0): if url not in self.visited: import heapq heapq.heappush(self.queue, (-priority, url)) # Від'ємне значення для max-heap async def crawl(self, session, max_urls=100): results = {} count = 0 import heapq while self.queue and count < max_urls: _, url = heapq.heappop(self.queue) if url in self.visited: continue self.visited.add(url) count += 1 try: html = await self.fetch_url(url, session) results[url] = html # Вилучення та визначення пріоритетів нових посилань new_urls = self.extract_links(html, url) for new_url, priority in new_urls: self.add_url(new_url, priority) except Exception as e: logger.error(f"Помилка сканування {url}: {e}") return results def extract_links(self, html, base_url): # Вилучення посилань та призначення пріоритетів на основі релевантності # Повернення списку кортежів (url, priority) pass ``` ### Регулювання на основі контенту Налаштування швидкості парсингу на основі типу контенту: ```python async def adaptive_scrape(url, session): """Адаптація поведінки парсингу на основі типу контенту.""" # Спочатку зробіть запит HEAD для перевірки типу контенту async with session.head(url) as head_response: content_type = head_response.headers.get('Content-Type', '') content_length = int(head_response.headers.get('Content-Length', 0)) # Налаштування поведінки на основі контенту if 'text/html' in content_type: # Стандартна HTML-сторінка await asyncio.sleep(1) # Стандартна затримка elif 'application/json' in content_type: # Кінцева точка API - може бути швидшою await asyncio.sleep(0.5) elif content_length > 1000000: # Великий файл - будьте обережнішими await asyncio.sleep(5) else: # Поведінка за замовчуванням await asyncio.sleep(2) # Тепер зробіть фактичний запит асинхронним з session.get(url) як відповіддю: return await response.text() ``` ## Висновок Оптимізація операцій парсингу - це баланс між продуктивністю, використанням ресурсів та етичними міркуваннями. Впроваджуючи методи, описані в цьому посібнику, ви можете створювати ефективні системи парсингу, які ефективно збирають дані, мінімізуючи вплив на цільові веб-сайти та ваші власні ресурси. Пам’ятайте, що найефективніший скрепер — це той, який: 1. Збирає лише те, що йому потрібно 2. Поважає ресурси цільового веб-сайту 3. Ефективно використовує обчислювальні ресурси 4. Грамотно обробляє помилки 5. Адаптується до змінних умов Завжди контролюйте свої операції зі скрепінгу та будьте готові коригувати свій підхід на основі показників продуктивності та відгуків цільових веб-сайтів. ---Останнє оновлення: 15.01.2025